\input{../main.tex}


\setmathstyle{\includegraphics[scale = 0.05]{../pics/utia-rest.png}}{Теория информации}{2 курс}


\begin{document}

\libproblem{inf-theory}{prefix-codes-2}

\begin{definition*}
    Для случайной величины $\alpha$ с вероятностями событий $(p_1, p_2, \dots)$ меру
    $$
        \entropy(\alpha) \coloneqq \sum p_i \log \frac{1}{p_i}.
    $$
    мы будем называть \deftext{энтропия} и обозначать $\entropy$ (иногда $h$).

    \deftext{Энтропией $\alpha$ при $\beta = b$} мы будем называть энтропию распределения $\alpha$ при
    условии, что $\beta = b$, то есть следующую величину:
    $$
        \entropy(\alpha \mid \beta = b) \coloneqq \sum_{i} \Pr[\alpha = i \mid \beta = b] \cdot \log
        \frac{1}{\Pr[\alpha = i \mid \beta = b]}
    $$ 
    
   	Тогда \deftext{энтропией $\alpha$ при условии $\beta$} мы назовем среднее значение по $b$ энтропии
    $\alpha$ при $\beta = b$. Таким образом:
    $$
        \entropy(\alpha \mid \beta) \coloneqq
        \Exp\limits_{b \sim \beta}[\entropy(\alpha \mid \beta = b)] =
        \sum_b \entropy(\alpha \mid \beta = b) \cdot \Pr[\beta = b].
    $$
\end{definition*}


\libproblem{inf-theory}{triple-indep}


\begin{definition*}
    \deftext{Взаимной информацией} между случайными величинами $\alpha$ и $\beta$ будем называть функцию
    $I(\alpha : \beta) \coloneqq \entropy(\alpha) - \entropy(\alpha \mid \beta)$.

    Также определим взаимную информацию в $\alpha$ и $\beta$ при условии $\gamma$.
    $I(\alpha : \beta \mid \gamma) = \entropy(\alpha \mid \gamma) - \entropy(\alpha \mid \beta, \gamma)$.
\end{definition*}

\libproblem{inf-theory}{information-properties}
\libproblem{inf-theory}{entropy-2-dim}
\libproblem{inf-theory}{markov-entropy}
% \libproblem{inf-theory}{injective-code}
\libproblem{inf-theory}{stone-lift}
\libproblem{inf-theory}{entropy-counter}



\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

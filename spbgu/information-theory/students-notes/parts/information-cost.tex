\subsection{Применения теории информации в коммуникационной сложности}

Начнем с урезанной коммуникационной модели. Определим функцию индексирования:
$$
    \Ind\colon [n] \times \{0,1\}^n \to \{0, 1\},\qquad \Ind(x, y) = y_x,
$$ 
где $y_x$~--- $x$-ый бит числа $y$. Рассмотрим следующую коммуникационную задачу, в которой Алиса
получает на вход число $x \in [n]$, а Боб получает $y \in \{0,1\}^n$. Им нужно найти
$\Ind(x, y)$. Нетрудно показать, что коммуникационная сложность этой задачи равна $\bigO{\log n}$, так как
Алиса может послать $x$, и по $x$ Боб может сказать бит, стоящий в $x$-ой позиции.

Усложним задачу. Пусть биты может посылать только Боб. Тогда коммуникационная сложность этой задачи
становится $\bigO{n}$, так как в итоге Боб должен послать информацию про все биты своего числа. Иначе у
Алисы может быть бит $x$, про который она ничего не знает.

Будем теперь считать, что Алиса и Боб получают входы согласно равномерному распределению среди всех
возможных входов. Посылать информацию может только Боб. Но теперь Бобу с Алисой разрешено делать ошибку
$\varepsilon \coloneqq \frac{1}{2} - \delta$, то есть не больше, чем на $\varepsilon \abs{X \times Y}$
входах можно сделать ошибку. Докажем, что коммуникационная сложность этой задачи равна
$\bigO{\delta^2 n}$. 

Рассмотрим коммуникационный протокол $\pi$, решающий нашу задачу. Заметим, что $\DCC^1(\pi) \ge \log
\abs{M}$, где $M$~--- множество листьев, а $\DCC^1$~--- коммуникационная сложность задачи, в 
которой только Боб может посылать биты. Рассмотрим энтропию $h(M)$ распределения на листьях, которая 
получается естественным способом из распределения на входах. Тогда нетрудно получить следующие
неравенства: 
$$
    \log{\abs{M}} \ge h(M) \ge I(M:y).
$$ 
По \textit{chain rule} мы получаем, что:
\begin{align*}
    I(M:y) &= \sum_i I(M:y_i \mid y_{x_{< i}})\\
           &= \sum_i h(y_i \mid y_{x_{< i}}) - h(y_i \mid M, y_{x_{\le i}})\\
           &= \sum_i h(y_i) - h(y_i \mid M, y_{x < i}) & \text{так как все $y_i$ независимы}\\
           &\ge \sum_i h(y_i) - h(y_i \mid M)\\
           &\ge \sum_i I(M : y_i).
\end{align*}

Для оценки $I(M : y_i)$ нам потребуется вспомогательная величина. Пусть $r_i^m$~--- вероятность
ошибки, при условии того, что Боб послал сообщение $m$ и $x = i$. Из определения $r_i^m$ следует, что
$\Exp\limits_{i, m}[r_i^m] \le \frac{1}{2} - \delta$.

Интуиция, скрывающаяся за следующими действиями такова: если $I(M : y_i)$ малая величина, то
сообщение Боба <<почти ничего>> не сообщает об $i$-ом бите и, как следствие, если у Алисы $x = i$, мы
получим ошибку с вероятностью примерно $\frac{1}{2}$. Попробуем формализовать эту стратегию.
\begin{align*}
  I(M : y_i) &= h(y_i) - h(y_i \mid M)\\
           &= 1 - h(y_i \mid M) & \text{поскольку распределение равномерное}\\
           &= 1 - \Exp_m[h(y_i \mid M = m)]\\
           &= 1 - \Exp_m[h(y_i \mid M = m, x = i)] & \text{$y_i$ и $M$ независимы относительно $x$}\\
           &= 1 - \Exp_m[h(r^m_i)].
\end{align*}
Теперь попробуем оценить всю сумму:
\begin{align*}
  \sum_i I(M : y_i) &= \sum_i 1 - \Exp_m[h(r^m_i)]\ge & \text{неравенство Йенсена}\\
                  &\ge \sum_i 1 - h\left(\Exp_m[r^m_i]\right) \\
                  &= n - n \sum_i\frac{1}{n} h\left(\Exp_m[r_i^m]\right) \ge & \text{неравенство Йенсена} \\
                  &\ge n - n h\left(\Exp_{i, m}[r_i^m]\right) \\
                  &\ge n \left(1 - h \left(\frac{1}{2} - \delta\right)\right)\\
                  &= \Omega(\delta^2n).
\end{align*}

Похожих идей будем придерживаться и в случае обычных коммуникационных протоколов.

\begin{definition}
    Пусть $f\colon X \times Y \to Z$ и $\mu$~--- распределение на $X \times Y$. Заметим, что для любого
    коммуникационного протокола $\Pi$ для функции $f$ распределение $\mu$ индуцирует распределение на
    листьях данного протокола естественным образом. \deftext{Внешней информационной стоимостью} (или 
    \deftext{внешним информационным разглашением}) протокола $\Pi$ по распределению $\mu$ будем называть
    величину:
    $$
        \ICext[\mu](\Pi) \coloneqq I(\Pi(X, Y) : X, Y).
    $$
    Также определим внешнюю информационную сложность самой функции
    $\ICext[\mu](f) \coloneqq \min\limits_{\Pi} \ICext[\mu](\Pi)$.

    По аналогии мы может рассмотреть и \deftext{внутреннее информационное разшлашение} (или 
    \deftext{внутренюю информационную стоимость}).
    $$
        \ICint[\mu](\Pi) \coloneqq I(\Pi(X, Y) : X \mid Y) + I(\Pi(X, Y) : Y \mid X).
    $$
\end{definition}

Следующая теорема на дает связь между коммуникационной сложностью и новой метод сложности.

\begin{theorem}
    Пусть $\pi$~--- протокол некоторой коммуникационной задачи c мерой $\mu$ на входах. Тогда:
    $$
        \DCC(\pi) \ge \ICext[\mu](\pi) \ge \ICint[\mu](\pi).
    $$ 
\end{theorem}

\begin{proof}
    $\DCC(\pi) \ge \log(M) \ge h(\pi) \ge I(\pi : X, Y)$, где $h(\pi)$~--- энтропия от распределения на
    листьях. Откуда следует первое неравенство.
    
    Теперь докажем второе: $I(\pi : X, Y) \ge I(\pi : X \mid Y) + I(\pi : Y \mid X)$. По chain rule мы
    получаем, что $I(\pi : X, Y) = \sum\limits_i I(\pi_i : X, Y \mid \pi_{< i})$, где $\pi_i$~--- это
    случайная величина, соответствующая $i$-ому биту, переданному в протоколе. Мы хотим показать, что:
    $$
        I(\pi_i : X, Y \mid \pi_{< i}) \ge I(\pi_i : X \mid Y, \pi_{< i}) +
        I(\pi_i : Y \mid X, \pi_{< i}),
    $$

    Для доказательства мы заметим, что:
    \begin{align*}
      I(\pi_i : X, Y \mid \pi_{< i}) &\ge I(\pi_i : X \mid Y, \pi_{< i}) \\
      I(\pi_i : X, Y \mid \pi_{< i}) &\ge I(\pi_i : Y \mid X, \pi_{< i})
    \end{align*}
    Так как по $\pi_{< i}$ мы знаем в какой вершине протокола мы находимся после $i$ шагов, то в этой
    вершине либо Алиса либо Боб посылают бит и $\pi_i$ будет определяться однозначно либо по $X$, либо по
    $Y$. Откуда следует, что одна из величин $I(\pi_i : X \mid Y, \pi_{< i})$, $I(\pi_i : Y \mid X,
    \pi_{< i})$ равна $0$. А поскольку $I(\pi_i : X, Y \mid \pi_{< i})$ не меньше второй величины, то мы
    получили требуемое неравенство.

    По chain rule получаем, что:
    $$
        \sum I(\pi_i : X \mid Y, \pi_{< i}) + I(\pi_i : Y \mid X, \pi_{< i}) =
        I(\pi : X \mid Y) + I(\pi : Y \mid X).
    $$ 
    Окуда следует второе неравенство из условия теоремы.
\end{proof}

При этом  для любого протокола $\pi$ существует такая мера $\mu$, что $\log \abs{\pi} = \ICext[\mu](\pi)$,
где $L$~--- число листье в протоколе.

\begin{theorem}[Храпченко]
    $L(\oplus_n) \ge \bigO{n^2}$, где функция $\oplus_n$ равна чётности количества единиц в записи числа
    длины $n$.
\end{theorem}

\begin{proof}
    Рассмотрим $\KW_{\oplus_n}$. Покажем, что $\ICint[\mu](\KW_{\oplus_n}) \ge 2 \log n$, откуда будет
    следовать, что $\DCC(\oplus_n) \ge 2 \log n$ и по теореме \ref{th:KW-theorem} мы получим $L(\oplus_n)
    \ge n^2$.

    Определим распределение $\mu$: равномерное распределение на парах $(x, x \oplus e_i) $, по всем $x
    \in \oplus_n^{-1}(1)$ (по всем $x$, в записи которых нечётное число единиц). Тогда:
    \begin{align*}
        I(\pi : Y \mid X) &= I(\pi: x \oplus e_I \mid X) = I(\pi : e_I \mid X) = h(e_I \mid X) - h(\pi
                            \mid X, e_I) \\
        &= h(e_I \mid X) = h(e_I) = \log n.
    \end{align*}
    Аналогично со вторым слагаемым.
\end{proof}

Данная оценка является точной.

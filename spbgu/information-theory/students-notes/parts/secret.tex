\section{Приложения теории информации к криптографии}

О криптографических применениях нам будет удобно говорить в терминах <<взаимной информации>>.

\begin{definition}
    \deftext{Взаимной информацией} между случайными величинами $\alpha$ и $\beta$ будем называть
    величину:
    $$
        I(\alpha : \beta) \coloneqq \entropy(\alpha) - \entropy(\alpha \mid \beta).
    $$
        
    Также определим \deftext{взаимную информацию в $\alpha$ и $\beta$ при условии $\gamma$}:
    $$
        I(\alpha : \beta \mid \gamma) \coloneqq \entropy(\alpha \mid \gamma) -
        \entropy(\alpha \mid \beta, \gamma).
    $$
\end{definition}

Взаимная информация обладает естественными свойствами:
\begin{enumerate}
    \item $I(\alpha : \beta) = I(\beta : \alpha)$;
    \item $\alpha$ и $\beta$ независимы тогда и только тогда, когда $I(\alpha : \beta) = 0$;
    \item $I(f(\alpha) : \beta) \le I(\alpha : \beta)$ для любой функции $f$.
    \item $I(\alpha : \beta) = \entropy(\alpha, \beta) - \entropy(\alpha \mid \beta) -
        \entropy(\beta \mid \alpha)$. 
\end{enumerate}
Доказательство, которых мы оставим в качестве упражнения.

\subsection{Шифрование с закрытым ключом}

Рассмотрим схему передачи слова от Алисы к Бобу. Мы предполагаем, что Алисе и Бобу заранее известен
некоторый ключ $k$, но этот ключ не известен злоумышленнику (Чарли).

Алиса получает на вход слово $w$. С помощью ключа $k$ и алгоритма $\alg{E}$ она кодирует слово $w$ и
отправляет получившееся слово $c \coloneqq \alg{E}(w, k)$ Бобу. Боб декодирует полученное слово с помощью
ключа $k$ и алгоритма дешифровки $\alg{D}$ и получает первоначальное слово $\alg{D}(c, k) = w$. Может
случиться так, что Чарли перехватил сообщение и получил слово $c$. Мы бы хотели, чтобы
он не смог восстановить исходное слово $w$, зная $c$, то есть: 
$$
    \begin{cases}
        \entropy(c \mid w, k) = 0, & \text{$c$ определяется значениями $w$ и $k$}\\
        \entropy(w \mid c, k) = 0, & \text{зная ключ $k$ и сообщение $c$ Боб может восстановить слово
            $w$}\\ 
        I(c : w) = 0. & \entropy(w) = \entropy(w \mid c)
\end{cases}
$$

Последнее условие нам говорит, что даже при том, что Чарли знает $c$, для восстановления $w$ ему
необходимо столько же битов информации, как и без знания $c$. То есть знание $c$ никак не помогло Чарли.
Если выполнены все три условия, то назовём эту схему \deftext{идеальной}.

\begin{remark}
    Первое условие можно опустить, что соответствует тому, что алгоритм шифрования $\alg{E}$ будем
    вероятностным.
\end{remark}

Рассмотрим пример идеальной схемы. Пусть $\alg{E}(w, k) = w \oplus k$, где слово $w$ выбирается из
множества слов длины $n$, $k$~--- это ключ, известный заранее, $|k| = n$. Заметим, что для этого
кодирования выполнены первые $2$ условия. Также несложно проверить третье условие.

В этой схеме ключевой момент, что нам потребовался ключ такой же длины, как и длина сообщения. Возникает
естественный вопрос можно ли сделать что-то более эффективное.

\begin{theorem}[Шеннон]
    Для идеальной схемы шифрования с закрытым ключом выполнено $\entropy(k) \ge \entropy(w)$, даже в
    случае вероятностного алгоритма шифрования.
\end{theorem}

\begin{proof}
    Доказательство следует из неравенств:
    $$
        \entropy(w) = \entropy(w \mid c) \le \entropy(w, k \mid c) = \entropy(k \mid c) +
        \entropy(w \mid c, k) = \entropy(k \mid c) \le \entropy(k).
    $$
\end{proof}

\subsection{Схема разделения секрета}

Пусть у нас есть некоторый секрет $S_0$ и $n$ участников и мы хотим раздать игрокам <<ключи>> $S_1,
\dots, S_n$ таким образом, чтобы они могли узнать секрет $S_0$ только все вместе, а любое подмножество
участников~--- не могло.

Попробуем переформулировать нашу задачу более формально. Пусть $(S_0, S_1, \dots, S_n)$~--- это набор
случайных величин. Будем называть его \deftext{схемой разделения секрета}, если: 
\begin{enumerate}
    \item $\entropy(S_0 \mid S_1, \dots, S_n) = 0$;
    \item $\entropy(S_0 \mid S_I) = \entropy(S_0)$, где $I \subsetneq [n]$.
\end{enumerate}

\begin{example}
    Пусть $S_0 \in \{0, 1\}^{\ell}$, $S_1, \ldots, S_{n - 1}$~--- это случайные величины, которые
    равномерно распределены на $\{0, 1\}^{\ell}$, и $S_n = \bigoplus\limits_{i = 0}^{n - 1}
    S_i$. Заметим, что $S_0$ определяется однозначно по остальным $S_i$, однако для любой перестановки
    $\sigma$ на $[n]$ элементах:
    $$
        \Pr[S_0 = a \mid S_{\sigma(1)}, \dots , S_{\sigma(n - 1)}] = \Pr[S_0 = a],
    $$ 
    и, следовательно, $\entropy(S_0 \mid S_{\sigma(1)}, \dots , S_{\sigma(n - 1)}) = \entropy(S_0)$.
\end{example}


Мы можем немного усложнить нашу задачу и потребовать, чтобы секрет могли открыть не все участники, а
любой кворум из $k$ участников. Пусть $(S_0, S_1, \dots, S_n)$~--- это набор случайных величин. Будем
называть его \deftext{схемой разделения секрета с порогом $k$}, если:
\begin{enumerate}
    \item $\entropy(S_0 \mid S_I) = 0$, где $|I| \ge k$;
    \item $\entropy(S_0 \mid S_I) = \entropy(S_0)$, где $|I| < k$.
\end{enumerate}

\paragraph{Схема Шамира.} Будем считать, что секрет $S_0$~--- это элемент некоторого конечного
$\field_q$. Зафиксируем набор различных точек $x_1, \dots, x_n \in \field_q$ (в частности это условие нам
говорит, что $q \ge n$). И рассмотрим многочлен:
$$
    P(x) = \sum_{i = 1}^{t - 1} a_ix^i + S_0,
$$
коэффициенты $a_i$ которого выберем случайным образом.

Пусть $i$-ый игрок получает число $S_i = P(x_i)$. Тогда любые $t$ игроков смогут интерполировать
многочлен и узнать $S_0$. Докажем, что $\entropy(S_0 \mid S_I) = \entropy(S_0)$, где $|I| < t$.

Пусть мы знаем значения $S_i$, то есть мы знаем значение $P(x_i) = c_i$, где $i \in I$. Это даёт нам
линейную систему уравнений на коэффициенты полинома $P$ с $|I|$ уравнениями и $t$ неизвестными. Заметим,
что:
$$
    \Pr[S_0 = S \mid \{P(x_i) = c_i\}]
$$
не зависит от значений $c_i$, поскольку все $x_i$ различны, а следовательно все уравнения линейно
независимы. Таким образом мы получаем, что:
$$
    \Pr[S_0 = S \mid \{P(x_i) = c_i\}] = \Pr[S_0 = S],
$$
и следовательно
$$
    \entropy(S_0 \mid S_I) = \entropy(S_0 \mid P(x_i)) =
    \Exp\limits_{c_i, i \in |I|}[\entropy(S_0 \mid \{P(x_i) = c_i\}_{i \in I})] = \entropy(S_0).
$$


\paragraph{Структуры доступа.} Естественный обобщением пороговой схемы является схема в которой заданы
некоторые произвольные <<авторизованные>> множества, которые могут узнать секрет, то есть у на задан
набор подмножеств $\Gamma \subseteq 2^{[n]}$, замкнутый вверх. Пусть $(S_0, S_1, \dots, S_n)$~--- это
набор случайных величин. Будем называть его
\deftext{схемой разделения секрета для структуры доступа $\Gamma$}, если:  
\begin{enumerate}
    \item $\entropy(S_0 \mid S_I) = 0$, где $|I| \in \Gamma$;
    \item $\entropy(S_0 \mid S_I) = \entropy(S_0)$, где $|I| \notin \Gamma$.
\end{enumerate}

\deftext{Идеальная схема разделения секрета}~--- это совершенная схема разделения секрета с
дополнительным требованием <<экономности>>.
$$
    \text{для всех } i \in \{1, 2, \dots, n\},\ \entropy(S_i) \le \entropy(S_0).
$$

\begin{lemma}
    Если участник $i$ является <<существенным>> в структуре доступа $\Gamma$ (т.е. существует такое
    $s \in \Gamma$, что $s \setminus \{i\} \notin \Gamma$), то $\entropy(S_i) \ge \entropy(S_0)$.
\end{lemma}

Доказательство оставим в качестве упражнения.
   
\begin{remark}
    Если $S_0$~--- равномерное распределение, то схема Шамира является идеальной.
\end{remark}

\begin{theorem}
    Существует такая структура доступа $\Gamma$, что для любой схемы разделения секрета выполнено
    неравенство $\max\limits_{i} \entropy(S_i) \ge \frac{n}{\log n} \entropy(S_0)$.
\end{theorem}
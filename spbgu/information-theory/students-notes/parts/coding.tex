\section{Теория кодирования}

\begin{definition}
    Будем называть \deftext{кодом} функцию $C\colon \{a_1, \dots , a_n\} \to \{0, 1\}^{*}$,
    сопоставляющую буквам некоторого алфавита кодовые слова. Если любое сообщение, которое получено
    применением кода $C$, декодируется однозначно (то есть единственным образом разрезается на образы
    $C$), то такой код будем называть \deftext{однозначно декодируемым}.
\end{definition}

\subsection{Префиксные коды}

Довольно удобно иметь более сильное свойство кода, чем однозначная декодируемость, которое позволяет
декодировать сообщение отдельно по буквам.

\begin{definition}
    Будем называть код \deftext{префиксным} (\deftext{беспрефиксным}, \deftext{prefix-free}), если
    никакое кодовое слово не является префиксом другого кодового слова.
\end{definition}

Давайте попробуем понять, что любой однозначно декодируемый код можно переделать в беспрефиксный. Для
этого мы попробуем описать критерии существования кодов.

\begin{theorem}
    \label{th:kraft-code}
    Пусть набор целых чисел $\ell_1, \dots, \ell_n$ удовлетворяет неравенству
    $$
        \sum\limits_{i = 1}^{n} 2^{-\ell_i} \le 1,
    $$
    тогда существует префиксный код с кодовыми словами $c_1, \dots , c_n$, где $|c_i| \le \ell_i$.
\end{theorem}

\begin{proof}
    Доказательство этого утверждения мы оставим в качестве упражнения.
\end{proof} 

Если мы покажем, что для любого однозначно декодируемого кода следующее неравенство всегда выполнено, то
вместе с теоремой \ref{th:kraft-code} это позволит переделывать одни коды в другие.

\begin{proposition}[Неравенство Крафта--Макмиллана]
    Для любого однозначно декодируемого кода c кодовыми словами $c_1, c_2, \dots, c_n$ выполнено
    неравенство
    $$
        \sum_{i = 1}^n 2^{-\abs{c_i}}\le 1.
    $$ 
\end{proposition}

\begin{proof}
    Доказательство этой теоремы должно использовать однозначную декодируемость <<в полном объёме>>, то
    есть для любой длины декодируемых сообщений. Формально заменим в каждом $c_i$ нули на буквы $x$, а
    единицы на буквы $y$, где $x$ и $y$ не коммутируют. Пусть $p_i(x, y)$~--- моном, соответствующий
    $c_i$ (например, коду $010$ соответствует моном $xyx$); $L$~--- большое натуральное число. Рассмотрим
    следующий полином:
    $$
        P^L(x, y) = \left(\sum_{i} p_i(x, y) \right)^L \le \sum_{i = L}^{L \cdot \max |c_i|} M_{i}(x, y),
    $$
    где $M_i$~--- это сумма всевозможных мономов степени $i$. Неравенство выполнено, так как код
    однозначно декодируемый, а значит каждый моном в левой части есть и в правой.

    Полагая $x = y = \frac{1}{2}$, получаем:
    $$
        P^L\left(\frac{1}{2},\frac{1}{2}\right) \le \sum_{i = L}^{L \cdot \max |c_i|}(2^i \cdot 2^{-i})
        \le O(L).
    $$
    Теперь предположим, что неравенство Крафта--Макмиллана не выполнено для данного кода. Тогда 
    $$
        \sum_i p_i\left(\frac{1}{2},\frac{1}{2}\right) = \sum 2^{-\abs{c_i}} = 1 + \varepsilon > 1.
    $$
    Значит, $P^L = (1 + \varepsilon)^{L} > O(L)$, что противоречит предыдущему рассуждению о линейности
    роста.
\end{proof}

Из предыдущих двух теорем следует, что по любому однозначно декодируемому коду можно построить префиксный
код с теми же длинами кодов.

\begin{theorem}[Шеннон]
    Для любого распределения $p$ и однозначно декодируемого кода выполнено неравенство
    $$
        \sum_i p_i\abs{c_i} \ge h(p),
    $$
    где $p_i$~--- вероятность, с которой встречается буква $i$, а $c_i$~--- её код. 
\end{theorem}

\begin{proof}
    Доказательство следует из неравенства Йенсена и неравенства Крафта--Макмиллана, 
    $$
        h(p) - \sum_i p_i\abs{c_i} = \sum_i p_i \cdot \log\frac{2^{-\abs{c_i}}}{p_i}
            \le \log\left(\sum_i p_i \cdot \frac{2^{-\abs{c_i}}}{p_i} \right) \le 0,
    $$
    что и требовалось.
\end{proof}

\begin{theorem}
    \label{th:prefix-code}
    Для любого распределения $p$ существует такой префиксный код, что
    $$
        \sum_i p_i\abs{c_i} \le h(p) + 1.
    $$
\end{theorem}

\begin{proof}
    Пусть $\abs{c_i} = \lceil \log\frac{1}{p_i} \rceil$. В таком случае неравенство из условия выполнено,
    так как $p_i|c_i| \leq p_i\log\frac{1}{p_i} + p_i$, и $ \sum p_i = 1$. Кроме того:
    $$
    \sum_i 2^{-\abs{c_i}} = \sum_i 2^{-\lceil \log\frac{1}{p_i}\rceil} \le \sum_i p_i = 1.
    $$
    Для завершения доказательства заметим, что по теореме \ref{th:kraft-code} существует префиксный код,
    удовлетворяющий этому неравенству. Этот код и будет удовлетворять условию теоремы.
\end{proof}


\subsection{Примеры эффективных кодов}

\paragraph{Код Шеннона--Фано.}
Отсортируем вероятности, $ p_1 \ge p_2 \ge \dots \ge p_n$. <<Уложим>> вероятности $p_i$ в отрезок $[0,
1]$, получая таким образом точки:
$$
0 \le p_1 < p_1 + p_2 < \dots < p_1 + p_2 + \dots + p_n \le 1.
$$ 

Разобьём интервал пополам, и скажем, что все коды, отвечающие точкам слева от разреза, начинаются с нуля,
а точкам справа~--- с единицы. Если отрезок пересекает разрез, и он самый левый (первый), то
соответствующий код начинается с $0$; если отрезок пересекает разрез и он самый правый (последний), то
код начинается с $1$. Иначе выбираем ноль или единицу произвольным образом. Продолжаем рекурсивно этот
процесс, пока в интервале не останется ровно один отрезок. 

\begin{exercise}
    Для кода Шеннона--Фано выполнено равенство:
    $$
    \sum_{i = 1}^n p_i\abs{c_i} = h(p) + O(1), \qquad n \to \infty.
    $$
\end{exercise}


\paragraph{Код Хаффмана.} Код Хаффмана строится индуктивно. При $n = 2$ кодовые слова~--- $c_1 = 0, c_2 =
1$. При $n > 2$ рассмотрим два символа $a$ и $b$ с минимальными вероятностями $p_{n - 1}$ и
$p_n$. Заменим указанные символы на новый символ $\sigma$, и дадим ему вероятность $p \coloneqq p_{n - 1}
+ p_n$. Построим код Хаффмана для $n - 1$ символа, и обозначим код символа $\sigma$ за $c$, после чего
скажем, что код символа $a$~--- это $c0$, а код символа $b$~--- $c1$.

\begin{theorem}[Хаффман]
    Для кода Хаффмана выполнено неравенство:
    \begin{equation}
        \label{eq:haffman-code-1}
        \sum_i p_i \abs{c_i} \le h(p) + 1,
    \end{equation}
    и для любого другого однозначно декодируемого кода $c_i'$ выполнено неравенство
    \begin{equation}
        \label{eq:haffman-code-2}
        \sum p_i\abs{c_i'} \ge \sum p_i\abs{c_i}.
    \end{equation}
\end{theorem}

\begin{proof}
    Докажем неравенство \eqref{eq:haffman-code-2}, тогда из него и теоремы \ref{th:prefix-code} будет
    следовать неравенство \eqref{eq:haffman-code-1}.

    Предположим, что есть некоторый префиксный код, для которого оно нарушено. Рассмотрим такой код с
    минимальным числом символов. Посмотрим на два символа с самым длинным кодом. Мы хотим сказать, что
    они имеют самые маленькие вероятности. Действительно, если бы это было не так, то коды символа с
    большей вероятностью и меньшей можно было бы поменять местами, при этом средняя длина кода от этого
    бы только уменьшилась. Можно считать, что коды этих двух символов равны $v0$ и $v1$
    соответственно(упражнение). <<Склеим>> эти два символа, как в коде Хаффмана.

    Получившийся код мы можем переделать в код Хаффмана так, чтобы средняя длина не увеличилась. Это
    можно сделать, так как мы брали код с минимальным числом символов, для которого нарушается
    неравенство. Осталось заметить, что если <<расклеить>> символы, то мы получим в точности 
    код Хаффмана, так как в нём мы делали то же самое первое действие (склеивали вершины с минимальной
    вероятностью). Отсюда следует, что наше предположение неверно, а значит неравенство
    \eqref{eq:haffman-code-2} выполнено всегда.
\end{proof}

\paragraph{Арифметическое кодирование.}
Назовём стандартным интервалом интервал вида $[0.v0, 0.v1)$, где $v$~--- некоторая последовательность
битов. Уложим вероятности $p_i$ в отрезок $[0, 1]$, получатся точки
$$
0 \le p_1 <  p_1 + p_2 < \dots < p_1 + p_2 + \dots + p_n \le 1.
$$ 

Пусть $[0.v_i0, 0.v_i1)$~--- максимальный стандартный интервал в отрезке
$$
[p_1 + p_2 + \dots + p_{i - 1}, p_1 + p_2 + \dots + p_{i}].
$$ 
Тогда сопоставим $i$-ой букве код $v_i0$. Заметим, что код получился префиксным, так как если $v_i$
является префиксом $v_j$, то интервал $[0.v_j0, 0.v_j1)$ вложен в интервал $[0.v_i0, 0.v_i1)$, а такого
при построении $ v_i $ не может произойти.

\begin{lemma}
    \label{lm:standard-interval}
    В отрезке $[a, b]$ длина наибольшего стандартного интервала не меньше, чем $\frac{b - a}{8}$.
\end{lemma}
\begin{proof}
    Доказательство мы оставим в качестве упражнения.
\end{proof}

\begin{proposition}
    Для арифметического кода выполняется неравенство:
    $$
        \sum_i p_i\abs{v_i}\le h(p)+2.
    $$
\end{proposition}
\begin{proof}
    Из леммы \ref{lm:standard-interval} следует, что если $\abs{v_i} = k$, то:
    $$
    0.v_i1 - 0.v_i0 = 2^{-k-1}  \ge \frac {p_i}{8}.
    $$ 
    Отсюда следует, что ${k + 1} \le \log \frac{8}{p_i}$, а значит
    $\abs{v_i} = k \le \log\frac{1}{p_i} + 2$, и
    $$
        \sum_i p_i \abs{v_i} \le h(p) + 2(p_1 + p_2 + \dots + p_n) \le h(p) + 2.
    $$
\end{proof}


\subsection{Кодирование с ошибками}

Пусть $p_1, \dots, p_k $~--- вероятности, с которыми встречаются буквы в алфавите. Будем рассматривать
слова фиксированной длины $n$, которые будут кодироваться в слова заданной длины $L_n$. Пусть нам даны
функции кодирования и декодирования:
$$
    E\colon [k]^n \to \{0, 1\}^{L_n}, \qquad D\colon \{0, 1\}^{L_n} \to [k]^n.
$$
При этом мы отказываемся от условия, что код декодируется однозначно, но требуем, чтобы вероятность
$  
\varepsilon_n \coloneqq \Pr[D(E(w)) \ne w]$ стремилась к нулю при $n \to \infty$.

\begin{theorem}[Шеннон]
    \begin{enumerate}
        \item Если $ L_n = \lceil h\cdot n\rceil$, где $h > h(p)$, то существуют такие функции $E, D$,
            что $\varepsilon_n \to 0$.
        \item Если $L_n = \lceil h \cdot n \rceil$, где $h < h(p)$, то для любых $E, D$
            последовательность $\varepsilon_n$ стремится к единице.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Пусть $w$~--- слово длины $n$. Будем говорить, что буква $i$ является \deftext{$\delta$-типичной},
    если $\abs{n_i / n - p_i} \le \delta$ где $n_i$~--- количество букв $i$ в $w$. Соответственно, $w$
    будем называть \deftext{$\delta$-типичным}, если это неравенство выполняется для всех букв
    $i$. Зафиксируем $\delta \coloneqq n^{-0.49}$ и рассмотрим случайную величину $X_{ij}$~---
    характеристическую функцию того, что в позиции $j$ находится буква $i$. Тогда для случайной величины:
    $X_i \coloneqq \sum_j X_{ij}$ мы можем написать неравенство Чебышёва:
    $$
        \Pr[\abs{X_i - \mu} \ge \delta n] \le \frac{\operatorname{Var}[X_i]}{(\delta n)^2} =
        \frac{n p_i (1 - p_i)}{(\delta n)^2} = \bigO{n^{-0.02}},
    $$
    где $\mu \coloneqq \Exp[X_i] = np_i$. Таким образом, доля слов, в которых буква $i$ нетипична
    стремится к нулю, а поскольку число букв фиксировано мы можем заключить, что и в целом доля
    нетипичных слов стремится к нулю. 
    
    Число слов с заданным количеством вхождений каждой буквы равно:
    $$
        N = \frac{n!}{n_1!n_2!\dots n_k!} = h(p) \cdot n + o(n),
    $$
    где $n_i = p_in$.\footnote{Это тождество было на практике.}
    
    Применим оценку $n! = \poly(n) \cdot (n / e)^n$. Тогда:
    $$
        \log{N} =
        \log\left(\left(\frac{n}{n_1}\right)^{n_1} \left(\frac{n}{n_2}\right)^{n_2} \!\ldots\;
          \left(\frac{n}{n_k}\right)^{n_k} \right)
        + \bigO{\log n}.
    $$

    Оценим это выражение:
    \begin{multline*}
        \log\left(\left(\frac{n}{n_1}\right)^{n_1} \left(\frac{n}{n_2}\right)^{n_2}
          \!\ldots\;\left(\frac{n}{n_k}\right)^{n_k} \right)
        = \sum n(p_i + \delta_i) \log\frac{1}{p_i + \delta_i}\\
        \le n h(p) + \bigO{\delta_i n} < h \cdot n.
    \end{multline*}
    
    Так как это типичное слово, то $|\delta_i| = |n_i - np_i| \le \delta n$ и следовательно количество
    типичных слов не превосходит
    $$
        2^{n h(p) + \bigO{\delta n} + \bigO{\log n}} < 2^{h \cdot n}.
    $$

    Теперь перейдем к доказательству второго случая: $h < h(p)$.
    
    Пусть $\varepsilon_n'$ вероятность ошибки при декодировании $\delta$-типичных слов. Нам достаточно
    показать, что $\varepsilon_n' \to 1$, поскольку $|\varepsilon'_n - \varepsilon|$ не 
    превосходит вероятности того, что слово нетипично, т.е. не более $\bigO{n^{-0.02}}$.
    
    Давайте рассмотрим конкретное $\delta$-типичное слово $w$. Оценим вероятность появления $w$:
    $$
        \Pr[w] = p_1^{n_1} \cdot \dots \cdot p_k^{n_k}
        = 2^{-\sum n_i \log\frac{1}{p_i}} \le 2^{-\sum (p_i + \delta_i) \log\frac{1}{p_i} \cdot n}.
    $$

    Всего мы можем корректно закодировать не более $2^{L_n}$ различных $\delta$-типичных слов,
    т.е. вероятность корректно декодировать $\delta$-типичное слово
    $$
        1 - \varepsilon'_n \le 2^{L_n} \cdot 2^{-h(\alpha) \cdot n + \bigO{\delta \cdot n}} \le
        2^{h \cdot n - h(\alpha) \cdot n + \bigO{\delta \cdot n}} \to 0.
    $$
\end{proof}

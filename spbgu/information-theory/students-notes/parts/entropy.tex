\section{Информация по Шеннону}

\subsection{Определение и свойства}
В прошлом разделе мы увидели ряд проблем, возникающих при работе с информацией по Хартли. С одной
стороны, у нас есть задача про $27$ монет и $3$ взвешивания, которую мы не понимаем как решать. С
другой~--- определение <<условной информации>> $(\chi_{Y \mid X}(A))$ плохо описывает наше
множество. Например, для следующих множеств выполнено равенство
$\chi_{y \mid x}(A) = \chi_{y \mid x}(B)$, хотя сами множества ничем не похожи друг на друга (даже с
точки знания количества объектов в них).
\begin{figure}[h]
	\centering
	\begin{subfigure}[h]{0.4\textwidth}
        \input{pics/set-ex-1.tex}
		\caption{} 
	\end{subfigure}
	\qquad\qquad
	\begin{subfigure}[h]{0.4\textwidth}
        \input{pics/set-ex-2.tex}
		\caption{} 
	\end{subfigure}
\end{figure}

Попробуем обобщить понятие информации для решения данных проблем. Введём новую меру информации $\mu$,
согласованную с определением по Хартли. Раньше мы предполагали, что все элементы в множестве $A$
одинаковы. Теперь предположим, что каждый элемент появляется с некоторой вероятностью $p_n$; то есть
$\mu$ будет задаваться уже не на множестве, а на распределении. В этих терминах свойство согласованности
можно выразить следующим образом: 
\begin{enumerate}
    \item $\mu(U_n) = \log{n}$, где $U_n$~--- равномерное распределение $n$ объектов (это и есть
        согласованность с предыдущим определением);
    \item $\mu(p) \ge 0$, где $p$~--- любое распределение;
    \item $\mu(p, q) = \mu(p) + \mu(q)$, где $p$ и $q$~--- независимые  распределения.
\end{enumerate}

Мы можем дополнить этот набор аксиом свойством <<непрерывности>>, а также утверждением
<<согласованности>> с определением условной вероятности. Тогда набор аксиом можно переписать в следующем
виде:
\begin{enumerate}
    \item \textit{монотонность}: если $M, M'$~--- равномерные распределения на $m \geq m'$ объектах
        соответственно, то $\mu(M) \geq \mu(M')$.
    \item \textit{аддитивность}: $\mu(p, q) = \mu(p) + \mu(q)$, где $p$ и $q$~--- независимые
        распределения;
    \item \textit{непрерывность}: мера $\mu(B_p)$ непрерывна по $p$, где $B_p$~--- распределение
        нечестной монетки, которая выпадает решкой с вероятностью $p$, и орлом с вероятностью $1 - p$;
    \item \textit{согласованность с условной вероятностью}:
        $$
            \mu(B, X) = \mu(B) + \Pr[B = 0] \cdot \mu(X \mid B = 0) + \Pr[B = 1] \cdot \mu(X \mid B = 1),
        $$
        где $B$~--- распределение нечестной монетки, $X$~--- произвольное распределение и $\mu(\cdot
        \mid \cdot)$ означает применение меры к условному распределению.
\end{enumerate}

В таком случае можно доказать (мы этого делать не будем), что мера $\mu$ с точностью до мультипликативной
константы определяется по формуле $\mu(X) \coloneqq \sum p_i \log \frac{1}{p_i}$.

\begin{definition}
    Для случайной величины $\alpha$ с вероятностями событий $(p_1, p_2, \dots)$ меру
    $$
        \entropy(\alpha) \coloneqq \sum p_i \log \frac{1}{p_i}.
    $$
    мы будем называть \deftext{энтропия} и обозначать $\entropy$ (иногда $h$).
\end{definition}

Рассмотрим простые примеры.

\begin{enumerate}
    \item Равномерное распределение: вероятность выпадения каждого элемента равна $\frac{1}{n}$.
        $$
            \entropy(U_n) = \sum_{k = 1}^n \frac{1}{n} \log n = \log n.
        $$
    \item Нечестная монетка:
        $$
            \entropy(B_p) = p \log \frac{1}{p} + (1 - p) \log \frac{1}{1 - p}
        $$
        --- \textit{бинарная энтропия}. Её часто обозначают через $\entropy(p)$.
\end{enumerate}


Поскольку теорему Шеннона мы оставили без доказательства, то нужно проверить, что энтропия удовлетворяет
нашим аксиомам.
\begin{proposition}
    Энтропия $\entropy(\alpha)$ обладает следующими свойствами:
    \begin{enumerate}
        \item $\entropy(\alpha) \le \log |\alpha|$, где $\alpha$~--- произвольное распределение и
            $|\alpha|$~--- размер носителя;
        \item $\entropy(\alpha, \beta) \le \entropy(\alpha) + \entropy(\beta)$, где $\alpha, \beta$~---
            произвольные распределения.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Оба пункта мы будем доказывать похожим способом~--- при помощи неравенства Йенсена.
    \begin{enumerate}
        \item Распишем по определению и применим неравенство:
            $$
                \entropy(\alpha) = \sum_{i = 1}^n p_i \log \frac{1}{p_i} \le
                \log\left(\sum_{i = 1}^n p_i \frac{1}{p_i} \right) = \log{n}
                = \log |\alpha|.
            $$
        \item Положим $p_{ij} = \Pr[\alpha = i, \beta = j]$, $p_{i \cdot} = \Pr[\alpha = i]$, $p_{*j} =
            \Pr[\beta = j]$. Заметим, что
            $$
                p_{i \cdot} = \sum_{j} p_{ij},\qquad p_{\cdot j} = \sum_{i}p_{ij},
            $$
            --- вероятность того, что выпал элемент $i$, равна вероятности того, что выпал элемент $i$ и
            какой-то элемент $j$ в $\beta$.

            В этих терминах мы можем описать энтропию пары:
            $$
                \entropy(\alpha, \beta) = \sum_{i, j} p_{ij} \cdot \log \frac{1}{p_{ij}},
            $$
            а также выразить сумму энтропий:
            $$
                \entropy(\alpha) + \entropy(\beta) = \sum_i p_{i \cdot} \cdot \log \frac{1}{p_{i \cdot}}
                + \sum_j p_{\cdot j} \log\frac{1}{p_{\cdot j}} =
                \sum_{ij} \left(p_{ij} \cdot \log \frac{1}{p_{i\cdot}} +
                p_{ij} \cdot \log \frac{1}{p_{\cdot j}} \right).
            $$

            Тогда по неравенству Йенсена:
            $$
                \entropy(\alpha, \beta) - \entropy(\alpha) - \entropy(\beta) =
                \sum_{ij} p_{ij} \log \frac{p_{i \cdot} p_{\cdot j}}{p_{ij}} \le
                \log\left( \sum_{i, j} p_{i \cdot} p_{\cdot j} \right) = \log 1 = 0.
            $$

            Отметим, что если $\alpha$ и $\beta$ независимы, то $p_{i \cdot} p_{\cdot j} = p_{ij}$, и мы
            получаем равенство $\entropy(\alpha, \beta) = \entropy(\alpha) + \entropy(\beta)$. 
    \end{enumerate}
\end{proof}

Теперь перейдем к определению условной энтропии.

\begin{definition}
    \deftext{Энтропией $\alpha$ при $\beta = b$} мы будем называть энтропию распределения $\alpha$ при
    условии, что $\beta = b$, то есть следующую величину:
    $$
        \entropy(\alpha \mid \beta = b) \coloneqq \sum_{i} \Pr[\alpha = i \mid \beta = b] \cdot \log
        \frac{1}{\Pr[\alpha = i \mid \beta = b]}
    $$ 
    
   	Тогда \deftext{энтропией $\alpha$ при условии $\beta$} мы назовем среднее значение по $b$ энтропии
    $\alpha$ при $\beta = b$. Таким образом:
    $$
        \entropy(\alpha \mid \beta) \coloneqq
        \Exp\limits_{b \sim \beta}[\entropy(\alpha \mid \beta = b)] =
        \sum_b \entropy(\alpha \mid \beta = b) \cdot \Pr[\beta = b].
    $$
\end{definition}

Условная энтропия обладает естественными базовыми свойствами.

\begin{proposition}
    \begin{enumerate}
        \item $\entropy(\alpha \mid \beta) \ge 0$, $\entropy(f(\alpha) \mid Y) = 0$.
        \item $\entropy(\alpha, \beta) = \entropy(\alpha) + \entropy(\beta \mid \alpha)$.
        \item $\entropy(\alpha) \ge \entropy(\alpha \mid \beta)$.
    \end{enumerate}
\end{proposition}

Мы приведем доказательство третьего свойства, а первое и второе оставим в качестве упражнения.
\begin{proof}
    По неравенству Йенсена для логарифма:
    $$
        \entropy(\alpha \mid \beta) - \entropy(\alpha) =
        \sum_{i, j} \left( p_{ij} \log \frac{1}{p_{i \mid j}} - 
        p_{ij} \log\frac{1}{p_{i\cdot}} \right) =
        \sum_{i,j} p_{ij}\cdot\log\frac{p_{i\cdot}}{p_{i \mid j}} \le 0,
    $$
    где $p_{i \mid j} = \Pr[\alpha = i \mid \beta = j].$
\end{proof}

Данное свойство обобщается естественным образом на условную энтропию.

\task{
	Докажите, что $\entropy(\alpha \mid \beta) \ge \entropy(\alpha \mid \beta, \gamma)$.
}


\subsection{Применения энтропии}

\paragraph{Еще немного о взвешивании.} Вернемся к последней задаче из раздела \ref{sec:fake-coin}. Теперь
мы готовы решить про $14$ монеток и $3$ взвешивания.

Предположим, что в стратегии при трёх равенствах мы получаем монету с номером $i$. Как мы помним, нельзя
определить, тяжелее она или легче других монет, в то время как для всех остальных монет это узнать
можно. Это значит, что в дереве решения (в нём $27$ листьев), $i$ встречается среди листьев только один
раз, а остальные индексы~--- два раза, причём один раз в ветке меньше, а в другой раз в ветке больше.

Зададим следующее распределение на монетках:
$$
    p_k =
    \begin{cases}
        \frac{1}{27}, &\text{если } k = i,\\
        \frac{2}{27}, &\text{если } k \ne i,\\
    \end{cases}
$$    
и распределение на парах из монетки и больше/меньше: $ p_{(k, <)} = p_{(k, >)} = p_k / 2$.

Заметим, что если существует дерево решений с тремя взвешиваниями, то распределение $p$ индуцирует
равномерное распределение на листьях $\ell$. С другой стороны мы знаем, что лист дерева определяется  
результатами трёх взвешиваний, назовём их $q_1, q_2, q_3$. Таким образом:
\begin{align*}
  \entropy(\ell) &= 3 \log 3\\
  \entropy(\ell \mid q_1, q_2, q_3) &= 0.
\end{align*}

Скомбинируем все вместе:
$$
    3\log 3 = \entropy(\ell) \le \entropy(q_1, q_2, q_3) + \entropy(\ell \mid q_1, q_2, q_3) =
    \entropy(q_1, q_2, q_3) \le \entropy(q_1) + \entropy(q_2) + \entropy(q_3).
$$
    
Однако $\entropy(q_j) \le \log 3$, так как мы выбираем из трёх вариантов, а энтропия не превосходит
логарифма размера носителя. Таким образом, единственная возможность, когда неравенство выполнено, это
когда все три исхода равновероятны. А это значит, что на первом шаге мы с равной вероятностью идём по
трём разным веткам от вершины дерева.

Пусть на первом шаге взвешивается по $k$ монет на каждой чаше. Вероятность того, что левая чаша
перевесила, равна $\frac{2k}{27}$, так как либо фальшивая монета легче и лежит слева, либо она тяжелее и
лежит справа; а вероятность того, что на левой чаше оказалась фальшивая монета легче настоящей равна
вероятности того, что на правой чаше оказалась фальшивая монета тяжелее настоящей и равна $k / 27$. Чтобы
это число равнялось одной трети, нужно взять $k = 4.5$, что невозможно. Противоречие.


\paragraph{Оценка на биномиальные коэффициенты.}
Теперь попробуем получить оценку на биномиальные коэффициенты при помощи свойств энтропии.

\begin{proposition}[Oценка на биноминальные коэффициенты]
    \label{prop:binomial-coef}
	Для произвольного $n$ и $k \le n / 2$ выполнено неравенство
    $$
        C = \sum_{i = 0}^k \binom{n}{i} \le 2^{n \entropy\left(\frac{k}{n}\right)}.
    $$
\end{proposition}

\begin{proof}
    Рассмотрим $n$ объектов, из них выберем не более, чем $k$ штук. Пусть $X$ соответствует равномерному
    распределению по таким множествам. Как следствие $\entropy(X) = \log C$. При этом:
    $$
        \entropy(X) \le \entropy\left(X_1, X_2, X_3, \dots, X_i\right) \le \sum \entropy(X_i),
    $$
    где $X_i$~--- вероятность того, что мы выбрали $i$-ый элемент (проекция распределения $X$ на $i$
    координату). По построению все эти распределения одинаковы, таким образом $\entropy(X) \le
    n\entropy(X_1)$. Но заметим, что вероятность, с которой мы можем выбрать первый элемент, не больше,
    чем $\frac{k}{n}$, то есть $\entropy(X_1) = h\left(\frac{k}{n}\right)$ (так как мы берём
    распределение на множествах мощности не более $k$). Откуда следует искомое неравенство.
\end{proof}


\paragraph{<<Треугольники>> и <<углы>> в графах.}
Пусть дан ориентированный граф без кратных рёбер и петель. Упорядоченную тройку $(x, y, z)$ вместе в
рёбрами из $x$ в $y$, из $y$ в $z$, и из $z$ в $x$, будем называть
\deftext{треугольником}. \deftext{Углом} мы будем называть упорядоченную тройку вершин $(x, y, z)$ вместе
с рёбрами из $x$ в $y$ и из $x$ в $z$. В частности, любое ребро $(x, y)$ является углом, так как можно
взять $z = y$.

\begin{theorem}[\cite{KR11}]
    В любом графе число треугольников не превосходит числа углов.
\end{theorem}
\begin{proof}
    Пусть $X, Y, Z$~--- случайные величины, соответствующие первой, второй и третьей вершине треугольника
    в равномерном распределении на треугольниках соответственно. Тогда
    $\entropy(X, Y, Z) = \log |\pmb{\triangle}|$. С другой стороны:
    $$
        \entropy(X, Y, Z) = \entropy(X) + \entropy(Y, Z \mid X) = \entropy(X) + \entropy(Y \mid X) +
        \entropy(Z \mid X, Y).
    $$
    Заметим, что если убрать $X$ из $\entropy(Z \mid X, Y)$, то энтропия только возрастёт. Следовательно:
    $$
        \entropy(X, Y, Z) \le \entropy(X) + \entropy(Y \mid X) + \entropy(Z \mid Y).
    $$
    Картинка симметрична (можно получить одно распределение из другого циклическим сдвигом), поэтому
    $\entropy(Y \mid X) = \entropy(Z \mid Y)$ и 
    $$
        \entropy(X, Y, Z) \le \entropy(X) + 2\entropy(Y \mid X).
    $$
    Определим распределение на углах. Выбираем вершину с той же вероятностью, с которой она является
    первой вершиной некоторого треугольника, обозначаем её через $x$. Выбираем равновероятно какой-то
    треугольник с вершиной $x$, проводим ребро и обозначаем вторую вершину $y$. Потом ещё раз независимо
    выбираем треугольник и проводим ребро в $z$. Посчитаем энтропию:
    $$
        \entropy(X, Y, Z) = \entropy(X) + \entropy(Y \mid X) + \entropy(Z \mid X,Y).
    $$
    Поскольку при известном $X$ величины $Y$ и $Z$ независимы, то $\entropy(Z \mid X, Y) = \entropy(Z \mid
    X)$. Поскольку $Y$ и $Z$ выбираются одинаковым образом, $\entropy(Y \mid X) = \entropy(Z \mid
    X)$. Таким образом,
    $$
    \entropy(X, Y, Z) = \entropy(X) + 2\entropy(Y \mid X).
    $$
    Осталось заметить, что в обоих распределениях $X$ выбирается одинаковым образом, и $Y$ при
    известном $X$ тоже выбирается также. Значит, энтропия некоторого распределения на углах не менее
    $\log |\pmb{\triangle}|$. Значит, углов не меньше, чем треугольников.
\end{proof}

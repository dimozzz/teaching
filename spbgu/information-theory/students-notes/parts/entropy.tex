\section{Информация по Шеннону}

\subsection{Определение и свойства}
В прошлом разделе мы увидели ряд проблем, возникающих при работе с информацией по Хартли. С одной
стороны, у нас есть задача про $27$ монет и $3$ взвешивания, которую мы не понимаем как решать. С другой~
--- определение <<условной информации>> $(\chi_{Y \mid X}(A))$ плохо описывает наше множество. Например, для
следующих множеств выполнено равенство $\chi_{y \mid x}(A) = \chi_{y \mid x}(B)$, хотя сами множества
ничем не похожи друг на друга (даже с точки знания количества объектов в них).  
\begin{figure}[h]
	\centering
	\begin{subfigure}[h]{0.4\textwidth}
        \input{pics/set-ex-1.tex}
		\caption{} 
	\end{subfigure}
	\qquad\qquad
	\begin{subfigure}[h]{0.4\textwidth}
        \input{pics/set-ex-2.tex}
		\caption{} 
	\end{subfigure}
\end{figure}

Попробуем обобщить понятие информации для решения данных проблем. Введём новую меру информации $\mu$,
согласованную с определением по Хартли. Раньше мы предполагали, что все элементы в множестве $A$
одинаковы. Теперь предположим, что каждый элемент появляется с некоторой вероятностью $p_n$; то есть
$\mu$ будет задаваться уже не на множестве, а на распределении. В этих терминах свойство согласованности
можно выразить следующим образом: 
\begin{enumerate}
    \item $\mu(U_n) = \log{n}$, где $U_n$~--- равномерное распределение $n$ объектов (это и есть
        согласованность с предыдущим определением);
    \item $\mu(p) \ge 0$, где $p$~--- любое распределение;
    \item $\mu(p, q) = \mu(p) + \mu(q)$, где $p$ и $q$~--- независимые  распределения.
\end{enumerate}

Мы можем дополнить этот набор аксиом свойством <<непрерывности>>, а также утверждением
<<согласованности>> с определением условной вероятности. Тогда набор аксиом можно переписать в следующем
виде:
\begin{enumerate}
    \item \textit{монотонность}: если $M, M'$~--- равномерные распределения на $m \geq m'$ объектах
        соответственно, то $\mu(M) \geq \mu(M')$.
    \item \textit{аддитивность}: $\mu(p, q) = \mu(p) + \mu(q)$, где $p$ и $q$~--- независимые
        распределения;
    \item \textit{непрерывность}: мера $\mu(B_p)$ непрерывна по $p$, где $B_p$~--- распределение
        нечестной монетки, которая выпадает решкой с вероятностью $p$, и орлом с вероятностью $1 - p$;
    \item \textit{согласованность с условной вероятностью}:
        $$
            \mu(B, X) = \mu(B) + \Pr[B = 0] \cdot \mu(X \mid B = 0) + \Pr[B = 1] \cdot \mu(X \mid B = 1),
        $$
        где $B$~--- распределение нечестной монетки, $X$~--- произвольное распределение и $\mu(\cdot
        \mid \cdot)$ означает применение меры к условному распределению.
\end{enumerate}

В таком случае можно доказать (мы этого делать не будем), что мера $\mu$ с точностью до мультипликативной
константы определяется по формуле $mu(X) \coloneqq \sum p_i \log \frac{1}{p_i}$.

\begin{definition}
    Для случайной величины $\alpha$ с вероятностями событий $(p_1, p_2, \dots)$ меру
    $$
        h(\alpha) \coloneqq \sum p_i \log \frac{1}{p_i}.
    $$
    мы будем называть \deftext{энтропия} и обозначать $h$ (иногда $H$).
\end{definition}

Рассмотрим простые примеры.

\begin{enumerate}
    \item Равномерное распределение: вероятность выпадения каждого элемента равна $\frac{1}{n}$.
        $$
            h(U_n) = \sum_{k = 1}^n \frac{1}{n} \log n = \log n.
        $$
    \item Нечестная монетка:
        $$
        h(B_p) = p \log \frac{1}{p} + (1 - p) \log \frac{1}{1 - p}
        $$
        --- \textit{бинарная энтропия}. Её часто обозначают через $h(p)$.
\end{enumerate}


Поскольку теорему Шеннона мы оставили без доказательства, то нужно проверить, что энтропия удовлетворяет
нашим аксиомам.
\begin{proposition}
    Энтропия $h(\alpha)$ обладает следующими свойствами:
    \begin{enumerate}
        \item $h(\alpha) \le \log |\alpha|$, где $\alpha$~--- произвольное распределение и $|\alpha|$~---
            размер носителя;
        \item $h(\alpha, \beta) \le h(\alpha) + h(\beta)$, где $\alpha, \beta$~--- произвольные распределения.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Оба пункта мы будем доказывать похожим способом~--- при помощи неравенства Йенсена.
    \begin{enumerate}
        \item Распишем по определению и применим неравенство:
            $$
                h(\alpha) = \sum_{i = 1}^n p_i \log \frac{1}{p_i} \le
                \log\left(\sum_{i = 1}^n p_i \frac{1}{p_i} \right) = \log{n}
                = \log |\alpha|.
            $$
        \item Положим $p_{ij} = \Pr[\alpha = i, \beta = j]$, $p_{i \cdot} = \Pr[\alpha = i]$, $p_{*j} =
            \Pr[\beta = j]$. Заметим, что
            $$
                p_{i \cdot} = \sum_{j} p_{ij},\qquad p_{\cdot j} = \sum_{i}p_{ij},
            $$
            --- вероятность того, что выпал элемент $i$, равна вероятности того, что выпал элемент $i$ и
            какой-то элемент $j$ в $\beta$.

            В этих терминах мы можем описать энтропию пары:
            $$
                h(\alpha, \beta) = \sum_{i, j} p_{ij} \cdot \log \frac{1}{p_{ij}},
            $$
            а также выразить сумму энтропий:
            $$
                h(\alpha) + h(\beta) = \sum_i p_{i \cdot} \cdot \log \frac{1}{p_{i \cdot}} +
                \sum_j p_{\cdot j} \log\frac{1}{p_{\cdot j}} =
                \sum_{ij} \left(p_{ij} \cdot \log \frac{1}{p_{i\cdot}} +
                p_{ij} \cdot \log \frac{1}{p_{\cdot j}} \right).
            $$

            Тогда по неравенству Йенсена:
            $$
                h(\alpha, \beta) - h(\alpha) - h(\beta) =
                \sum_{ij} p_{ij} \log \frac{p_{i \cdot} p_{\cdot j}}{p_{ij}} \le
                \log\left( \sum_{i, j} p_{i \cdot} p_{\cdot j} \right) = \log 1 = 0.
            $$

            Отметим, что если $\alpha$ и $\beta$ независимы, то $p_{i \cdot} p_{\cdot j} = p_{ij}$, и мы
            получаем равенство $h(\alpha, \beta) = h(\alpha) + h(\beta)$. 
    \end{enumerate}
\end{proof}

Теперь перейдем к определению условной энтропии.

\begin{definition}
    \deftext{Энтропией $\alpha$ при $\beta = b$} мы будем называть энтропию распределения $\alpha$ при
    условии, что $\beta = b$, то есть следующую величину:
    $$
        h(\alpha \mid \beta = b) \coloneqq \sum_{i} \Pr[\alpha = i \mid \beta = b] \cdot \log
        \frac{1}{\Pr[\alpha = i \mid \beta = b]}
    $$ 
    
   	Тогда \deftext{энтропией $\alpha$ при условии $\beta$} мы назовем среднее значение по $b$ энтропии
    $\alpha$ при $\beta = b$. Таким образом:
    $$
        h(\alpha \mid \beta) \coloneqq \Exp\limits_{b \sim \beta}[h(\alpha \mid \beta = b)] = \sum_b
        h(\alpha \mid \beta = b) \cdot \Pr[\beta = b].
    $$
\end{definition}

Условная энтропия обладает естественными базовыми свойствами.

\begin{proposition}
    \begin{enumerate}
        \item $h(\alpha X \mid \beta) \ge 0$, $h(f(\alpha) \mid Y) = 0$.
        \item $h(\alpha, \beta) = h(\alpha) + h(\beta \mid \alpha)$.
        \item $h(\alpha) \ge h(\alpha \mid \beta)$.
    \end{enumerate}
\end{proposition}

Мы приведем доказательство третьего свойства, а первое и второе оставим в качестве упражнения.
\begin{proof}
    По неравенству Йенсена для логарифма:
    $$
        h(\alpha \mid \beta) - h(\alpha) = \sum_{i, j} \left( p_{ij} \log \frac{1}{p_{i \mid j}} -
        p_{ij} \log\frac{1}{p_{i\cdot}} \right) =
        \sum_{i,j} p_{ij}\cdot\log\frac{p_{i\cdot}}{p_{i \mid j}} \le 0,
    $$
    где $p_{i \mid j} = \Pr[\alpha = i \mid \beta = j].$
\end{proof}

Данное свойство обобщается естественным образом на условную энтропию.
\begin{exercise}
	Докажите, что $h(\alpha \mid \beta) \ge h(\alpha \mid \beta, \gamma)$.
\end{exercise}

\subsection{Применение энтропии}

\paragraph{Еще немного о взвешивании.} Вернемся к последней задаче из раздела \ref{sec:fake-coin}. Теперь мы
готовы решить про $14$ монеток и $3$ взвешивания.

Предположим, что в стратегии при трёх равенствах мы получаем монету с номером $i$. Как мы помним, нельзя
определить, тяжелее она или легче других монет, в то время как для всех остальных монет это узнать
можно. Это значит, что в дереве решения (в нём $27$ листьев), $i$ встречается среди листьев только один
раз, а остальные индексы~--- два раза, причём один раз в ветке меньше, а в другой раз в ветке больше.

Зададим следующее распределение на монетках:
$$
    p_k =
    \begin{cases}
        \frac{1}{27}, &\text{если } k = i,\\
        \frac{2}{27}, &\text{если } k \ne i,\\
    \end{cases}
$$    
и распределение на парах из монетки и больше/меньше: $ p_{(k, <)} = p_{(k, >)} = p_k / 2$.

Заметим, что если существует дерево решений с тремя взвешиваниями, то распределение $p$ индуцирует
равномерное распределение на листьях $\ell$. С другой стороны мы знаем, что лист дерева определяется  
результатами трёх взвешиваний, назовём их $q_1, q_2, q_3$. Таким образом:
\begin{align*}
  h(\ell) &= 3 \log 3\\
  h(\ell \mid q_1, q_2, q_3) &= 0.
\end{align*}

Скомбинируем все вместе:
$$
    3\log 3 = h(\ell) \le h(q_1, q_2, q_3) +h(\ell \mid q_1, q_2, q_3) = h(q_1, q_2, q_3) \le h(q_1) +
    h(q_2) + h(q_3).
$$
    
Однако $h(q_j) \le \log 3$, так как мы выбираем из трёх вариантов, а энтропия не превосходит
логарифма размера носителя. Таким образом, единственная возможность, когда неравенство выполнено, это
когда все три исхода равновероятны. А это значит, что на первом шаге мы с равной вероятностью идём по
трём разным веткам от вершины дерева.

Пусть на первом шаге взвешивается по $k$ монет на каждой чаше. Вероятность того, что левая чаша
перевесила, равна $\frac{2k}{27}$, так как либо фальшивая монета легче и лежит слева, либо она тяжелее и
лежит справа; а вероятность того, что на левой чаше оказалась фальшивая монета легче настоящей равна
вероятности того, что на правой чаше оказалась фальшивая монета тяжелее настоящей и равна $k / 27$. Чтобы
это число равнялось одной трети, нужно взять $k = 4.5$, что невозможно. Противоречие.


\paragraph{Оценка на биноминальные коэффициенты.}
Теперь попробуем получить оценку на биномиальные коэффициенты при помощи свойств энтропии.

\begin{proposition}[Oценка на биноминальные коэффициенты]
	Для произвольного $n$ и $k \le n / 2$ выполнено неравенство
    $$
        C = \sum_{i = 0}^k \binom{n}{i} \le 2^{nh\left(\frac{k}{n}\right)}.
    $$
\end{proposition}

\begin{proof}
    Рассмотрим $n$ объектов, из них выберем не более, чем $k$ штук. Пусть $X$ соответствует равномерному
    распределению по таким множествам. Как следствие $h(X) = \log C$. При этом:
    $$
        h(X) \le h\left(\sum X_i\right) \le \sum h(X_i),
    $$
    где $X_i$~--- вероятность того, что мы выбрали $i$-ый элемент (проекция распределения $X$ на $i$
    координату). По построению все эти распределения одинаковы, таким образом $h(X) \le nh(X_1)$. Но
    заметим, что вероятность, с которой мы можем выбрать первый элемент, не больше, чем $\frac{k}{n}$, то
    есть $h(X_1) = h\left(\frac{k}{n}\right)$ (так как мы берём распределение на множествах мощности не
    более $k$). Откуда следует искомое неравенство.
\end{proof}


\paragraph{<<Треугольники>> и <<углы>> в графах.}
Пусть дан ориентированный граф без кратных рёбер и петель. Упорядоченную тройку $(x, y, z)$ вместе в
рёбрами из $x$ в $y$, из $y$ в $z$, и из $z$ в $x$, будем называть
\deftext{треугольником}. \deftext{Углом} мы будем называть упорядоченную тройку вершин $(x, y, z)$ вместе
с рёбрами из $x$ в $y$ и из $x$ в $z$. В частности, любое ребро $(x, y)$ является углом, так как можно
взять $z = y$.

\begin{theorem}[\cite{KR11}]
    В любом графе число треугольников не превосходит числа углов.
\end{theorem}
\begin{proof}
    Пусть $X, Y, Z$~--- случайные величины, соответствующие первой, второй и третьей вершине треугольника
    в равномерном распределении на треугольниках соответственно. Тогда
    $h(X, Y, Z) = \log |\pmb{\triangle}|$. С другой стороны:
    $$
        h(X, Y, Z) = h(X) + h(Y, Z \mid X) = h(X) + h(Y \mid X) + h(Z \mid X, Y).
    $$
    Заметим, что если убрать $X$ из $h(Z \mid X, Y)$, то энтропия только возрастёт. Следовательно:
    $$
        h(X, Y, Z) \le h(X) + h(Y \mid X) + h(Z \mid Y).
    $$
    Картинка симметрична (можно получить одно распределение из другого циклическим сдвигом), поэтому
    $h(Y \mid X) = H(Z \mid Y)$ и 
    $$
        h(X, Y, Z) \le h(X) + 2h(Y \mid X).
    $$
    Определим распределение на углах. Выбираем вершину с той же вероятностью, с которой она является
    первой вершиной некоторого треугольника, обозначаем её через $x$. Выбираем равновероятно какой-то
    треугольник с вершиной $x$, проводим ребро и обозначаем вторую вершину $y$. Потом ещё раз независимо
    выбираем треугольник и проводим ребро в $z$. Посчитаем энтропию:
    $$
        h(X, Y, Z) = h(X) + h(Y \mid X) + h(Z \mid X,Y).
    $$
    Поскольку при известном $X$ величины $Y$ и $Z$ независимы, то $h(Z \mid X, Y) = h(Z \mid
    X)$. Поскольку $Y$ и $Z$ выбираются одинаковым образом, $h(Y \mid X) = h(Z \mid X)$. Таким образом,
    $$
    h(X, Y, Z) = h(X) + 2h(Y \mid X).
    $$
    Осталось заметить, что в обоих распределениях $X$ выбирается одинаковым образом, и $Y$ при
    известном $X$ тоже выбирается также. Значит, энтропия некоторого распределения на углах не менее
    $\log |\pmb{\triangle}|$. Значит, углов не меньше, чем треугольников.
\end{proof}

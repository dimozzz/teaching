%2020-04-01

\chapter{Distributional protocols}
\section{Определения и свойства}
Пусть для каждого входа $(x, y)$ задана его вероятность $\mu(x, y)$.
Distributional protocol "--- \emph{детерминированный} протокол, который решает правильно задачу на всех входах, кроме доли, имеющей меру не более $\eps$.
Сложность протокола $\D^\mu_\eps(f)$ "--- это высота дерева.

\begin{theorem}
$\R^{pub}_\eps(f) = \max_{\mu} \D^{\mu}_\eps (f)$
\end{theorem}
\begin{proof}
$\geqslant$: Для любого $\mu$ мы можем из $\R_\eps$  получить протокол: выберем наилучшие случайные биты. У них будет ошибка не более меры $\eps$, иначе есть вход, у которого ошибка больше $\eps$ в протоколе $\R_\eps$.

$\leqslant$. Пусть $c = \max_{\mu} \D^{\mu}_\eps (f)$. Будем использовать минимакс-теорему (формулировку см. ниже).
Строки матрицы помечены всеми возможными входами $(x, y)$.
Столбцы помечены всеми детерминированными протоколами сложности не более $c$.
В клетке ставим $1$, если протокол верно работает на этом входе, и $0$ иначе.

Рассмотрим распределение на входах $p$ и вероятностный протокол $\Pi$, являющийся распределением $q$ на детерминированных протоколах. Тогда $p^TAq$ "--- это вероятность верного ответа $\Pi$ на распределении на входах $p$.

Мы знаем, что для любого распределения на входах $p$ найдётся даже детерминированный протокол с ошибкой не более $\eps$, поэтому $\min_p\max_q p^TAq\geqslant 1-\eps$.

По минимакс-теореме $\max_q\min_p p^TAq\geqslant 1-\eps$. То есть существует вероятностный протокол $\Pi$ такой, что какое бы распределение на входах не выбрать, вероятность верного ответа хотя бы $1-\eps$. Выбирая распределения, сконцентрированные на одном входе, получаем требуемое.
\end{proof}

\begin{theorem}[Минимакс-теорема, фон Нейман, 1928]
Пусть $A$ "--- матрица, Алиса выбирает строку, Боб выбирает столбец, Алиса получает число денег, написанное в клетке на пересечении.

Пусть теперь выбираем вероятности $p_i$ для строк и $q_j$ для столбцов. $\E[\text{выигрыш}] = p^TAq$.

Тогда $\max_{p} \min_{q} p^TAq  = \min_{q} \max_{p} p^TAq$.
\end{theorem}

\mycomment{\textcolor{red}{TODO: Добавить сюда комментарий про неправильную формулировку.}}

Нижние оценки на $\R(f)$ можно получать, выбирая распределение $\mu$ и доказывая нижнюю оценку.
Все известные нижние оценки на $\R(f)$, кроме лифтинга, так и устроены.

\section{Discrepancy}
Discrepancy "--- обобщение техники Rectangle Size. Пусть задано распределение на входах $\mu$.

\begin{definition}
Discrepancy прямоугольника $R$ "--- это $\disc_{\mu}(f, R) = \left|\mu(R\cap f^{-1}(0)) - \mu(R\cap f^{-1}(1)\right|$.

Discrepancy функции $f$ "--- это $\disc_\mu(f) = \max_R \disc_{\mu}(f, R)$ по всем прямоугольникам $R$.
\end{definition}

\begin{theorem}
$\D^\mu_{1/2-\eps}(f) = \Omega\left(\log\left(\frac{2\eps}{\disc_{\mu}(f)}\right)\right)$
\end{theorem}
\begin{proof}
Рассмотрим прямоугольник $R_i$ листа $l_i$. Пусть вероятность ошибки этого листа есть $\mathrm{err}_i$ (она не больше $\mu(R_i)/2$). Тогда $\disc(R_i) = (\mu(R_i) - \mathrm{err}_i) - \mathrm{err}_i = \mu(R_i) - 2\mathrm{err}_i$.

Рассмотрим $\sum_i \disc(R_i)$. С одной стороны, это $\sum (\mu(R_i) - 2\mathrm{err}_i) = 1 - 2\mathrm{err} = 1 - 2(1/2 - \eps) = 2\eps$.

С другой стороны, слагаемых в сумме $2^\D$, у каждого $\disc$ не больше $\disc(f)$. Поэтому $2\eps \leqslant 2^\D\cdot \disc(f)$, откуда $\D\geqslant \log\left(\frac{2\eps}{\disc(f)}\right)$.

\mycomment{На лекции предлагалось посмотреть на большой прямоугольник в листе, но доказательство было оставлено как упражнение. У меня не получилось реализовать этот план, оценка получается слабее.}

%Выберем самый большой прямоугольник в листе, пусть он размера $s$. Тогда всего листьев хотя бы $1/s$, тогда высота протокола хотя бы $\log(1/s)$.

%Если прямоугольник имеет $\disc = d$, то в нём хотя бы (по мере) $(s - d)/2$ и нулей, и единиц. Значит, $(s-d)/2\le \eps$, то есть $.

%Надо показать, что прямоугольник имеет размер не больше $\frac{2\eps}{\disc_{\mu}(f)}$.  этого прямоугольника не более $\disc_\mu(f)$, поэтому и нуле

%Если $\disc$ маленький, а прямоугольник в листе большой, то ошибаемся слишком много.
\end{proof}

\begin{theorem}
$\disc_{uni}(\IP) \leqslant 2^{-n/2}$.
\end{theorem}
\begin{proof}
Рассмотрим $\pm 1$-матрицу $A$ для $\IP$.

Рассмотрим прямоугольник $R = S\times T$. $\disc(\IP, R) = \left|\sum_{(x,y)\in R} (-1)^{\langle x, y\rangle}\right| / 2^{2n}$.

Числитель:
$|\One^T_S\cdot A\cdot \One_T| \leqslant
\lVert A\rVert_2 \lVert \One_S\rVert_2 \lVert \One_T\rVert_2 \leqslant
\lVert A\rVert_2 \sqrt{|S||T|} = 
\lVert A\rVert_2 \sqrt{|R|}
$.

$A^2 = 2^{n} E$ "--- проверили явно, см. второе доказательство теоремы~\ref{rk(M-IP) lower bound}.
Поэтому $\lVert A^2\rVert_2 = 2^n$, и $\lVert A\rVert_2=2^{n/2}$.

Итого числитель $\leqslant 2^{n/2}\sqrt{|R|} \leqslant 2^{3n/2}$, что и требовалось доказать.
\end{proof}

Из этого получаем нижнюю оценку на $\R(\IP)$:

\begin{theorem}
$\R(\IP)\geqslant \Omega(n)$.
\end{theorem}

Для $\DISJ$ доказать нижнюю оценку таким способом не получится:

\begin{proposition}
Для любого распределения $\mu$ $\disc_\mu(\DISJ)\geqslant \frac{1}{2n} - \frac{1}{2n^2}$.
\end{proposition}

То есть нижняя оценка на коммуникацию получится разве что логарифмическая.

\begin{proof}
Посмотрим на всю матрицу $M$.
Пусть $\disc(\DISJ, M) < \frac{1}{n}$.
Тогда мера нулей хотя бы $\frac{1}{2}-\frac{1}{2n}$.
Но у нулей есть покрытие $n$ 0-прямоугольниками.
Тогда какой-то из них имеет меру не менее $(\frac{1}{2}-\frac{1}{2n})/n$ и такое же $\disc$.
\end{proof}

\begin{theorem}[Babai et al.; 1986]
$\D^{uni}_{\eps}(\DISJ^{\leqslant \sqrt{n}}_n) = \Omega(\sqrt{n})$
\end{theorem}
\begin{proof}
Пусть $\eps < 1/100$.

Будем рассматривать (почти 1)-прямоугольники.
Пусть в прямоугольнике $S\times T$ не более $\eps$ нулей.
Покажем, что либо $\mu(S)\leqslant 2^{-c\sqrt{n}}$, либо $\mu(T)\leqslant 2^{-c\sqrt{n}}$.

(Продолжение в следующий раз.)
\end{proof}

Из этого получаем:

\begin{theorem}
$\R(\DISJ) = \Omega(\sqrt{n})$.
\end{theorem}

Для product-распределений это является и верхней оценкой.
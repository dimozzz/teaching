\chapter{Теория информации и коммуникационная сложность}

Предполагается, что читатель знаком с понятиями энтропии и взаимной информации, а также их простыми свойствами.

\section{Нижняя и верхняя оценка на $\IND$}

$\IND : [n] \times \{0, 1\}^n \rightarrow \{0, 1\}$. 

$\IND(x, y) = y_x$, т.е., у Боба есть массив из нулей и единиц, а у Алисы --- индекс в этом массиве, нужно узнать соответствующий элемент массива.

$D(\IND) = \log{n}$. А если только Боб может посылать сообщения? Детерминированная коммуникационная сложность будет $n$, это следует из простых комбинаторных соображений. А что с вероятностной сложностью? Будем, как и в конце прошлой главы, думать вместо вместо вероятностной сложности про distributional сложность.

Пусть у нас есть некоторый протокол $\Pi$. Тогда $cost(\Pi) \geq \log{supp(M)} \geq H(M)$, где $M$ --- это множество различных сообщений, которые может послать Боб. Причём эти неравенства верны для любого распределения на входах. Давайте выберем равномерное распределение, чтобы удобно применить chain rule.

$H(M) \geq I(M : y) \geq \sum_i I(M : y_i)$. Здесь во втором неравенство мы применили chain rule и опустили все лишние условия. Почему это легально? Распишем пример применения chain rule для двух переменных.

$I(x_1, x_2 : M) = I(x_1 : M) + I(x_2 : M \mid x_1)$, при этом $I(x_2 : M \mid x_1) = H(x_2 \mid x_1) - H(M \mid x_2, x_1) \geq H(x_2) - H(M \mid x_2) = I(x_2 : M)$ (воспользовались тем, что при равномерном распределении $H(x_2 : x_1) = H(x_2)$).

Обозначим $r_i^m = Pr[err \mid M = m, I = i]$. Условия здесь независимы, когда мы имеем дело с $1$-way коммуникацией.

$I(M : y_i) = H(y_i) - H(y_i \mid M) = 1 - E_m[H(y_i \mid M = m)] = 1 - E_m[H(y_i \mid M = m, I = i)]$, так как условия независимы. Это равно $1 - E_m[H(1 - r_i^m)] = 1 - E_m[H(r_i^m)]$. Применим неравенство Йенсена, получим, что это $\geq 1 - H(E_m[r_i^m])$.

Итого $cost(\Pi) \geq \sum I(M : y_i) \geq \sum_i (1 - H(E_m[r_i^m])) = n - n \sum \frac{1}{n} H(E_m[r_i^m]) \geq n - n H(E_{m, i}[r_i^m]) \geq n - n H(\frac{1}{2} - \delta) = \Omega(\delta^2n)$.

Получили нижнюю оценку. Самая простая верхняя получается следующим образом: Боб посылает Алисе $\delta n$ битов своей строки (случайных, но мы, как и всегда, можем зафиксировать лучшие). Если нужный бит там есть, Алиса выдаёт ответ. Если нужного бита там нет, Алиса бросает монетку и отвечает то, что выпало.

Получили верхнюю оценку порядка $\delta n$, а нижняя порядка $\delta^2 n$. Улучшим верхнюю.

Сгенерируем много случайных строк $s_1, \ldots, s_i, \ldots$ и пошлём Алисе первый индекс $j$ такой, что расстояние $\Delta(s_j, y) \leq \frac{1}{2} - \delta$. Ошибка таким образом не превышает $\frac{1}{2} - \delta$, нужно только доказать, что нам потребуется сгенерировать не очень много строк, прежде чем мы найдём подходящую.

Количество подходящих нам строк: $\sum_{i = 0}^{(\frac{1}{2} - \delta)n} {{n}\choose{i}} \approx 2^{H(\frac{1}{2} - \delta)n}$.

Вероятность насэмплить подходящую: $\frac{2^{H(\frac{1}{2} - \delta)n}}{2^n} = 2^{-n(1 - H(\frac{1}{2} - \delta))} \approx 2^{-n\delta^2}$. Значит, надо насэмплить порядка $2^{n\delta^2}$ строк.
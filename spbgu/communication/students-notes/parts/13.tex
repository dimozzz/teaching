\section{Теория информации и коммуникационная сложность}

Предполагается, что читатель знаком с понятиями энтропии и взаимной информации, а также их простыми свойствами.

\subsection{Нижняя и верхняя оценка на $\Ind$}

Начнем с урезанной коммуникационной модели. Определим функцию индексирования:
$$
    \Ind\colon [n] \times \{0,1\}^n \to \{0, 1\},\qquad \Ind(x, y) = y_x,
$$ 
где $y_x$~--- $x$-ый бит числа $y$. Рассмотрим следующую коммуникационную задачу, в которой Алиса
получает на вход число $x \in [n]$, а Боб получает $y \in \{0,1\}^n$. Им нужно найти
$\Ind(x, y)$. Нетрудно показать, что коммуникационная сложность этой задачи равна $\bigO{\log n}$, так
как Алиса может послать $x$, и по $x$ Боб может сказать бит, стоящий в $x$-ой позиции.

Усложним задачу. Пусть биты может посылать только Боб. Тогда коммуникационная сложность этой задачи
становится $\bigO{n}$, так как в итоге Боб должен послать информацию про все биты своего числа. Иначе у
Алисы может быть бит $x$, про который она ничего не знает.

Будем теперь считать, что Алиса и Боб получают входы согласно равномерному распределению среди всех
возможных входов. Посылать информацию может только Боб. Но теперь Бобу с Алисой разрешено делать ошибку
$\varepsilon \coloneqq \frac{1}{2} - \delta$, то есть не больше, чем на $\varepsilon \abs{X \times Y}$
входах можно сделать ошибку. Докажем, что коммуникационная сложность этой задачи равна
$\bigO{\delta^2 n}$. 

Рассмотрим коммуникационный протокол $\pi$, решающий нашу задачу. Заметим, что $\DCC^1(\pi) \ge \log
\abs{M}$, где $M$~--- множество листьев, а $\DCC^1$~--- коммуникационная сложность задачи, в 
которой только Боб может посылать биты. Рассмотрим энтропию $\entropy(M)$ распределения на листьях, которая 
получается естественным способом из распределения на входах. Тогда нетрудно получить следующие
неравенства: 
$$
    \log{\abs{M}} \ge \entropy(M) \ge I(M:y).
$$ 
По \textit{chain rule} мы получаем, что:
\begin{align*}
    I(M:y) &= \sum_i I(M:y_i \mid y_{< i})\\
           &= \sum_i \entropy(y_i \mid y_{< i}) - \entropy(y_i \mid M, y_{\le i})\\
           &= \sum_i \entropy(y_i) - \entropy(y_i \mid M, y_{< i}) & \text{так как все $y_i$ независимы}\\
           &\ge \sum_i \entropy(y_i) - \entropy(y_i \mid M)\\
           &\ge \sum_i I(M : y_i).
\end{align*}

Для оценки $I(M : y_i)$ нам потребуется вспомогательная величина. Пусть $r_i^m$~--- вероятность
ошибки, при условии того, что Боб послал сообщение $m$ и $x = i$. Из определения $r_i^m$ следует, что
$\Exp\limits_{i, m}[r_i^m] \le \frac{1}{2} - \delta$.

Интуиция, скрывающаяся за следующими действиями такова: если $I(M : y_i)$ малая величина, то
сообщение Боба <<почти ничего>> не сообщает об $i$-ом бите и, как следствие, если у Алисы $x = i$, мы
получим ошибку с вероятностью примерно $\frac{1}{2}$. Попробуем формализовать эту стратегию.
\begin{align*}
  I(M : y_i) &= \entropy(y_i) - \entropy(y_i \mid M)\\
           &= 1 - \entropy(y_i \mid M) & \text{поскольку распределение равномерное}\\
           &= 1 - \Exp_m[\entropy(y_i \mid M = m)]\\
           &= 1 - \Exp_m[\entropy(y_i \mid M = m, x = i)] & \text{$y_i$ и $M$ независимы относительно $x$}\\
           &= 1 - \Exp_m[\entropy(r^m_i)].
\end{align*}
Теперь попробуем оценить всю сумму:
\begin{align*}
  \sum_i I(M : y_i) &= \sum_i 1 - \Exp_m[\entropy(r^m_i)]\ge & \text{неравенство Йенсена}\\
                  &\ge \sum_i 1 - \entropy\left(\Exp_m[r^m_i]\right) \\
                  &= n - n \sum_i\frac{1}{n} \entropy\left(\Exp_m[r_i^m]\right) \ge & \text{неравенство
                                                                                      Йенсена} \\
                  &\ge n - n \entropy\left(\Exp_{i, m}[r_i^m]\right) \\
                  &\ge n \left(1 - \entropy \left(\frac{1}{2} - \delta\right)\right)\\
                  &= \Omega(\delta^2n).
\end{align*}


Получили нижнюю оценку. Самая простая верхняя получается следующим образом: Боб посылает Алисе $\delta n$
битов своей строки (случайных, но мы, как и всегда, можем зафиксировать лучшие). Если нужный бит там
есть, Алиса выдаёт ответ. Если нужного бита там нет, Алиса бросает монетку и отвечает то, что выпало.

Получили верхнюю оценку порядка $\delta n$, а нижняя порядка $\delta^2 n$. Улучшим верхнюю.

Сгенерируем много случайных строк $s_1, \dots, s_i, \dots$ и пошлём Алисе такой первый индекс $j$, что
расстояние $\Delta(s_j, y) \leq \frac{1}{2} - \delta$. Ошибка таким образом не превышает $\frac{1}{2} -
\delta$, нужно только доказать, что нам потребуется сгенерировать не очень много строк, прежде чем мы
найдём подходящую. 

Количество подходящих нам строк: $\sum\limits_{i = 0}^{(\frac{1}{2} - \delta)n} \binom{n}{i} \approx
2^{\entropy(\frac{1}{2} - \delta)n}$.

Вероятность насэмплить подходящую: $\frac{2^{\entropy(\frac{1}{2} - \delta)n}}{2^n} =
2^{-n(1 - \entropy(\frac{1}{2} - \delta))} \approx 2^{-n\delta^2}$. Значит, надо насэмплить порядка
$2^{n\delta^2}$ строк.
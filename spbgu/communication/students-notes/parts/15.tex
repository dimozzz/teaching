\section{Ещё одна нижняя оценка на $\DISJ$}

Теперь мы докажем оценку на $\DISJ$, пользуясь теорией информации.

\begin{theorem}
$IC^{int}(\DISJ) = \Omega(n)$.
\end{theorem}

Определим меру $\mu$ следующим образом: сначала равновероятно выбираем индекс $i$, в нём $X_i$ и $Y_i$ выбираются бросками честной монетки. В остальных индексах $(X_j, Y_j)$ выбираются из тройки $(0, 0)$, $(1, 0)$, $(0, 1)$ равновероятно.

Обозначим $D$ событие $X \cap Y = \varnothing$.

Рассмотрим $I(\Pi : X \mid Y, D) + I(\Pi : Y \mid X, D)$. Это не внутренняя информация относительно меры $\mu$. Однако давайте рассмотрим дополнительное условие $D$ как некоторый модификатор того, как устроена наша мера. То есть выражение выше --- это внутренняя информация протокола относительно \textit{некоторой другой меры}, и она тоже даёт нижнюю оценку на коммуникационную сложность. Если $I(\Pi : X \mid Y, D) + I(\Pi : Y \mid X, D)$ является $\Omega(n)$, то всё отлично, теорема доказана. Предположим, что это не так. Заметим, что из-за симметрии нам достаточно анализировать только одно из слагаемых.

Итак, предположим, что $I(\Pi : X \mid Y, D) < \gamma n$. Со значением $\gamma$ определимся позже --- мы подберём его так, что получится противоречие.

$I(\Pi : X \mid Y, D) = \sum_i I(\Pi : X_i \mid X_{< i}, Y, D) \geq \sum_i I(\Pi : X_i \mid X_{< i}, Y_{\geq i}, D) = \sum_i I(\Pi : X_i \mid X_{< i}, Y_{\geq i}, J = i, D) = n E[I(\Pi : X_i \mid X_{< i}, Y_{\geq i}, J = i, D)] = n I(\Pi : X_j \mid X_{< j}, Y_{\geq j}, j, D)$.

Здесь первое равенство --- это chain rule. Неравенство получается за счёт того, что мы опустили условия, независимые с $X_i$. Далее можно заметить, что условие $J = i$ никак не влияет на распределение, если нам уже известно, что произошло $D$, так что его можно дописать в каждое слагаемое суммы. Последние два равенства используют простые свойства взаимной информации.

Тогда из $I(\Pi : X \mid Y, D) < \gamma n$ следует $I(\Pi : X_j \mid X_{< j}, Y_{\geq j}, j, D) < \gamma$.

Из этого мы хотим вывести, что протокол будет работать с большой ошибкой. Начнём со следующей леммы:

\begin{lemma}
$I(\Pi : X_j \mid X_{< j}, Y_{\geq j}, j, D) \geq \frac{2}{3}I(\Pi : X_j \mid X_{< j}, Y_{> j}, j)$.
\end{lemma}

\begin{proof}

$I(\Pi : X_j \mid X_{< j}, Y_{\geq j}, j, D) \geq Pr[Y_j = 0 \mid D] \cdot I(\Pi : X_j \mid X_{< j}, Y_{> j}, Y_j = 0, j, D) \geq \frac{2}{3} \cdot I(\Pi : X_j \mid X_{< j}, Y_{> j}, j)$.

Первое неравенство из формулы полной вероятности. Затем стираем $D$ из условий, потому что оно уже произошло, после этого можем стереть из условий $Y_j = 0$, так как $X_j$ не зависит от этого условия.

\end{proof}

Итак, если $I(\Pi : X_j \mid X_{< j}, Y_{\geq j}, j, D) < \gamma$, то $I(\Pi : X_j \mid X_{< j}, Y_{> j}, j) < \gamma'$, $\gamma' \coloneqq \frac{2}{3}\gamma$.

Сделаем небольшое отступление про условную взаимную информацию.

$I(A : B \mid C) = E_c[I(A : B \mid C = c)] = E_{b, c}[D(p(A \mid b, c) || p(A \mid c))] \geq 2(E_{b, c}[|p(A \mid b, c) - p(A \mid c)|])^2$. Здесь $D$ --- это divergence.

\mycomment{\textcolor{blue}{Дисклеймер: предполагалось, что это некоторый общеизвестный факт??, так что я даже не пыталась понять, правда это или нет.}}

Из этого следует, что условная взаимная информация даёт нам оценку на статистическое расстояние некоторых распределений.

Пусть $z \coloneqq (\Pi, X_{< j}, Y_{> j}, j)$.

Тогда $I(\Pi : X_j \mid X_{< j}, Y_{> j}, j) \geq 2(E_z[|p(X_j \mid z) - p(X_j \mid X_{< j}, Y_{> j}, j)|])^2$. Но так как $X_j$ не зависит от битов входа с другими номерами, то во втором распределении можно оставить только условие $j$. Здесь можно заметить, что $p(X_j \mid j)$ --- это равномерное распределение, и от распределения $p(X_j \mid z)$ до него не очень большое расстояние, а именно:

$E_z[|p(X_j \mid z) - p(X_j \mid j)|] < \sqrt{\frac{3}{4}\gamma}$.

Назовём $z$ хорошим, если $|p(X_j \mid z) - p(X_j \mid j)| \leq 10 E[\ldots]$.

Тогда $Pr[z \text{ is good}] \geq \frac{9}{10}$ по неравенству Маркова.

Если $z$ хорошее, то распределения очень близки. Пусть $\Pi$ отвечает, что множества не пересекаются (другой случай аналогично, с ещё более хорошей оценкой). Тогда $Pr[\text{err} \mid z \text{ is good}] \geq Pr[X_i = 1, Y_i = 1 \mid z \text{ is good}, j = i] = Pr[X_i = 1 \mid z \text{ is good}] \cdot Pr[Y_i = 1 \mid z \text{ is good}] \geq \frac{1}{2} - 10\sqrt{\frac{3}{4}\gamma}$. Если $\gamma$ достаточно маленькая, то эта величина больше $\frac{1}{9}$.

Тогда вероятность ошибки $Pr[\text{err}] = \frac{1}{9} \cdot Pr[z \text{ is good}] \geq \frac{1}{10}$, что плохо, если мы хотим ошибку меньше.

Теорема доказана.


\section{Внешняя и внутренняя информационная сложность}

Пусть у нас есть некоторый коммуникационный протокол $\Pi$, который находит значение $f(x, y)$ с ошибкой $\delta$ относительно меры $\mu$. Определим внешнюю информационную сложность протокола $IC^{ext}_{\mu, \delta} \coloneqq I(X, Y : \Pi)$.

А также внутреннюю информационную сложность: $IC^{int}_{\mu, \delta}(\Pi) \coloneqq I(X : \Pi \mid Y) + I(Y : \Pi \mid X)$.

Далее при записи чаще всего будем опускать индексы $\mu$ и $\delta$.

\begin{lemma}
$cost(\Pi) \geq IC^{ext}(\Pi) \geq IC^{int}(\Pi)$.
\end{lemma}

\begin{proof}
Первое неравенство: $IC^{ext}(\Pi) \leq H(\Pi) \leq \log{size(\Pi)} \leq cost(\Pi)$.

Второе неравенство: по индукции по количеству пересланных битов. База: $I(\varnothing : X, Y) = I(\varnothing : X \mid Y) + I(\varnothing : Y \mid X)$. Пусть $T_i$ --- первые $i$ битов в протоколе, $b_i$ --- $i$-й бит протокола, и сейчас раунд Алисы.

$I(T_i : X, Y) = I(T_{i - 1} : X, Y) + I(b_i : X, Y \mid T_{i - 1}) = I(T_{i - 1} : X, Y) + I(b_i : Y \mid T_{i - 1}) + I(b_i : X \mid Y, T_{i - 1}) \geq I(T_{i - 1} : X, Y) + I(b_i : X \mid Y, T_{i - 1})$.

Дальше применим предположение индукции и воспользуемся тем фактом, что $I(b_i : Y \mid X, T_{i - 1}) = 0$, так как сейчас раунд Алисы. Получается, что $I(T_{i - 1} : X, Y) + I(b_i : X \mid Y, T_{i - 1}) \geq I(T_{i - 1} : X \mid Y) + I(T_{i - 1} : Y \mid Y) + I(b_i : X \mid Y, T_{i - 1}) + I(b_i : Y \mid X, T_{i - 1}) = I(T_i : X \mid Y) + I(T_i : Y \mid X)$.
\end{proof}

Докажем ещё одну небольшую теорему, которая пригодится нам позже.

\begin{theorem}
$I(X, Y : \Pi) = I(X, Y : \Pi, R) = I(X, Y : \Pi \mid R)$, где $R$ --- это публичные случайные биты, которые используются в протоколе.
\end{theorem}

\begin{proof}
$I(X, Y : \Pi, R) = I(\Pi : X, Y) + I(R : X, Y \mid \Pi)$. Второе слагаемое равно $H(X, Y \mid \Pi) - H(X, Y \mid \Pi, R) = 0$, так как случайные биты являются частью протокола $\Pi$. Так мы доказали первое равенство.

Второе равенство: $I(X, Y : \Pi, R) = I(X, Y : R) + I(X, Y : \Pi \mid R)$. Первое слагаемое равно $0$, так как случайные биты не могут сообщить нам никакой информации о входах.
\end{proof}


\section{Direct sum}

Пусть мы хотим $n$ раз посчитать функцию $f$ на каких-то входах. Если мы знаем, что $R(f) = r$, то что можно сказать про $R(f^n)$? Правда ли, что нельзя сделать ничего лучше, чем $n$ раз посчитать функцию $f$ за $r$ бит?

В общем случае это неправда. Пример: $R^{pr}(\EQ_n) = \mathcal{O}(\log{n})$, но $R^{pr}(\EQ^l_n) = \mathcal{O}(l \log{l} + \log{n})$ (чтобы это получить, воспользуемся теоремой Ньюмана). Если $l \approx \log{n}$, то $l \log{l} + \log{n} \approx \log{n} \log{\log{n}}$ получается существенно меньше, чем $l \log{n} \approx \log^2{n}$.

Однако можно доказать следующее:

\begin{theorem}
$IC^{int}_{\mu^n, \delta}(f^n) \geq n \cdot IC^{int}_{\mu, \delta}(f)$.
\end{theorem}

\begin{proof}
Для простоты пока будем считать, что $\mu$ --- product measure.

Пусть вход Алисы для функции $f$ $A$, вход Боба $B$. Возьмём протокол $\Pi$ для $f^n$ и сделаем следующее:

\begin{itemize}
    \item выберем $j \in [n]$ равновероятно;
    \item $X_{< j}$, $Y_{< j}$ насэмплим публичными случайными битами;
    \item $X_{> j}$, $Y_{> j}$ насэмплим приватными случайными битами.
\end{itemize}

Тогда вход для функции $f^n$: $X \coloneqq (X_{< j}, A, X_{> j})$, $Y \coloneqq (Y_{< j}, B, Y_{> j})$.

$\Pi'$ --- протокол, который генерирует этот вход и затем запускает на нём протокол $\Pi$.

Будем доказывать $I(A : \Pi' \mid B) \leq \frac{1}{n}I(X : \Pi \mid Y)$, это эквивалентно условию теоремы.

$I(A : \Pi' \mid B) = I(A : J, X_{< j}, Y_{< j}, \Pi \mid B)$. Далее, так как мы можем весьма вольно обращаться с публичными случайными битами, это равно $I(A : \Pi \mid B, X_{< j}, Y_{< j}, J)$. $B$ сэмплится из того же распределения, что и $Y_{< j}$, так что можно просто написать $Y_{\leq j}$, а также добавить в условия и приватные случайные биты $Y_{> j}$ --- они независимы с $A$, так что взаимная информация от этого может только увеличиться.

Получили $I(A : \Pi \mid X_{< j}, Y, J)$. Это равно $\frac{1}{n}\sum I(X_j : \Pi \mid X_{< j}, Y, J = j) = \frac{1}{n}I(X : \Pi \mid Y)$.

Единственное место, где мы пользовались тем, что наша мера является product measure --- это возможность насэмплить $X_{> j}$ и $Y_{> j}$ приватными случайными битами независимо друг от друга. Тогда изменим наш протокол следующим образом: теперь $X_{< j}$, $Y_{> j}$ --- это публичные случайные биты, а $X_{> j}$, $Y_{< j}$ --- приватные. Теперь приватные биты можно насэмплить в соответствии с нужной мерой, зная публичные. В доказательстве, соответственно, нужно только заменить $Y_{< j}$ на $Y_{> j}$.

\end{proof}


\section{Внешняя и внутренняя информационная сложность}

Похожих идей будем придерживаться и в случае обычных коммуникационных протоколов.

\begin{definition}
    Пусть $f\colon X \times Y \to Z$ и $\mu$~--- распределение на $X \times Y$.

    Заметим, что для любого коммуникационного протокола $\Pi$ для функции $f$ распределение $\mu$
    индуцирует распределение на листьях данного протокола естественным образом. Под распределением $\Pi$
    будем подразумевать пару, состоящую из распределения на листьях и публичных случайных битов протокола.
    
    \deftext{Внешней информационной стоимостью} (или \deftext{внешним информационным разглашением})
    протокола $\Pi$ по распределению $\mu$ будем называть величину:
    $$
        \ICext[\mu](\Pi) \coloneqq I(\Pi(X, Y) : X, Y).
    $$
    Также определим внешнюю информационную сложность самой функции
    $\ICext[\mu](f) \coloneqq \min\limits_{\Pi} \ICext[\mu](\Pi)$.

    По аналогии мы может рассмотреть и \deftext{внутреннее информационное разглашение} (или 
    \deftext{внутренюю информационную стоимость}).
    $$
        \ICint[\mu](\Pi) \coloneqq I(\Pi(X, Y) : X \mid Y) + I(\Pi(X, Y) : Y \mid X).
    $$
\end{definition}

Далее при записи чаще всего будем опускать индекс $\mu$, если он монятен из контекста. Заметим, простое
свойство, которое нам позволит убирать и добавлять публичные случайные биты, когда потребуется.

\begin{theorem}
    $I(X, Y : \Pi) = I(X, Y : \Pi, R) = I(X, Y : \Pi \mid R)$, где $R$ --- это публичные случайные биты,
    которые используются в протоколе.
\end{theorem}

\begin{proof}
    $I(X, Y : \Pi, R) = I(\Pi : X, Y) + I(R : X, Y \mid \Pi)$. Второе слагаемое равно $H(X, Y \mid \Pi) -
    H(X, Y \mid \Pi, R) = 0$, так как случайные биты являются частью протокола $\Pi$. Так мы доказали
    первое равенство.

    Второе равенство: $I(X, Y : \Pi, R) = I(X, Y : R) + I(X, Y : \Pi \mid R)$. Первое слагаемое равно
    $0$, так как случайные биты не могут сообщить нам никакой информации о входах.
\end{proof}


Следующая теорема на дает связь между коммуникационной сложностью и новой мерой сложности.

\begin{theorem}
    Пусть $\Pi$~--- протокол некоторой коммуникационной задачи c мерой $\mu$ на входах. Тогда:
    $$
        \DCC(\Pi) \ge \ICext[\mu](\Pi) \ge \ICint[\mu](\Pi).
    $$ 
\end{theorem}

\begin{proof}
    Докажем первое неравенство. Заметим, что $\Pi = (M, R)$, где $M$~--- сообщение переданное в
    протоколе, а $R$~--- публичные случайные биты. Заметим, что
    $$
        I(\Pi : X, Y) = I(M, R : X, Y) = I(R : X, Y) + I(M : X, Y \mid R) = I(M : X, Y \mid R),
    $$
    но $\DCC(\Pi) \ge \log(M) \ge \entropy(M \mid R) \ge I(M : X, Y \mid R)$. Откуда следует первое
    неравенство.

    Теперь докажем второе: $I(\Pi : X, Y) \ge I(\Pi : X \mid Y) + I(\Pi : Y \mid X)$. Зафиксируем
    публичные случайные биты протокола (это необязательно, но упростит нотацию). По chain rule мы
    получаем, что $I(\Pi : X, Y) = \sum\limits_i I(\Pi_i : X, Y \mid \Pi_{< i})$, где $\Pi_i$~--- это
    случайная величина, соответствующая $i$-ому биту, переданному в протоколе. Мы хотим показать, что:
    $$
        I(\Pi_i : X, Y \mid \Pi_{< i}) \ge I(\Pi_i : X \mid Y, \Pi_{< i}) +
        I(\Pi_i : Y \mid X, \Pi_{< i}),
    $$

    Для доказательства заметим, что для любого $m \in \{0, 1\}^{i - 1}$:
    \begin{align*}
      I(\Pi_i : X, Y \mid \Pi_{< i} = m) &\ge I(\Pi_i : X \mid Y, \Pi_{< i} = m) \\
      I(\Pi_i : X, Y \mid \Pi_{< i} = m) &\ge I(\Pi_i : Y \mid X, \Pi_{< i} = m)
    \end{align*}
    Так как по $m$ мы знаем в какой вершине протокола мы находимся после $i$ шагов, то в этой вершине
    либо Алиса либо Боб посылают бит и $\pi_i$ будет определяться однозначно либо по $X$, либо по
    $Y$. Откуда следует, что одна из величин $I(\Pi_i : X \mid Y, \Pi_{< i} = m)$, $I(\Pi_i : Y \mid X,
    \Pi_{< i} = m)$ равна $0$. А поскольку $I(\Pi_i : X, Y \mid \Pi_{< i} = m)$ не меньше второй
    величины, то мы получаем, что:
    $$
        I(\Pi_i : X, Y \mid \Pi_{< i} = m) \ge I(\Pi_i : X \mid Y, \Pi_{< i} = m) +
        I(\Pi_i : Y \mid X, \Pi_{< i} = m),
    $$
    откуда требуемое неравенство получается усреднением $m$ по мере $\mu$.

    По chain rule получаем, что:
    $$
        \sum I(\Pi_i : X \mid Y, \Pi_{< i}) + I(\Pi_i : Y \mid X, \Pi_{< i}) =
        I(\Pi : X \mid Y) + I(\Pi : Y \mid X).
    $$ 
    Окуда следует второе неравенство из условия теоремы.
\end{proof}


\section{Direct sum}

Пусть мы хотим $n$ раз посчитать функцию $f$ на каких-то входах. Если мы знаем, что $\RCC(f) = r$, то что
можно сказать про $\RCC(f^n)$? Правда ли, что нельзя сделать ничего лучше, чем $n$ раз посчитать функцию
$f$ за $r$ бит?

В общем случае это неправда. Пример: $\Rpri(\EQ_n) = \bigO{\log{n}}$, но $\Rpri(\EQ^{\ell}_n) =
\bigO{\ell \log{\ell} + \log{n}}$ (чтобы это получить, воспользуемся теоремой Ньюмана). Если $\ell
\approx \log{n}$, то $\ell \log{\ell} + \log{n} \approx \log{n} \log{\log{n}}$ получается существенно
меньше, чем $\ell \log{n} \approx \log^2{n}$. 

Однако можно доказать следующее:

\begin{theorem}
    $\ICint[\mu^n, \delta](f^n) \geq n \cdot \ICint[\mu, \delta](f)$.
\end{theorem}

\begin{proof}
    Для простоты пока будем считать, что $\mu$ --- product measure.

    Пусть вход Алисы для функции $f$ $A$, вход Боба $B$. Возьмём протокол $\Pi$ для $f^n$ и сделаем
    следующее:
    \begin{itemize}
        \item выберем $j \in [n]$ равновероятно;
        \item $X_{< j}$, $Y_{< j}$ насэмплим публичными случайными битами;
        \item $X_{> j}$, $Y_{> j}$ насэмплим приватными случайными битами.
    \end{itemize}

    Тогда вход для функции $f^n$: $X \coloneqq (X_{< j}, A, X_{> j})$, $Y \coloneqq (Y_{< j}, B,
    Y_{> j})$.

    $\Pi'$ --- протокол, который генерирует этот вход и затем запускает на нём протокол $\Pi$. Будем
    доказывать $I(A : \Pi' \mid B) \le \frac{1}{n} I(X : \Pi \mid Y)$, это эквивалентно условию теоремы.

    $I(A : \Pi' \mid B) = I(A : J, X_{< j}, Y_{< j}, \Pi \mid B)$. Далее, так как мы можем весьма вольно
    обращаться с публичными случайными битами, это равно $I(A : \Pi \mid B, X_{< j}, Y_{< j}, J)$. $B$
    сэмплится из того же распределения, что и $Y_{< j}$, так что можно просто написать $Y_{\leq j}$, а
    также добавить в условия и приватные случайные биты $Y_{> j}$ --- они независимы с $A$, так что
    взаимная информация от этого может только увеличиться.

    Получили $I(A : \Pi \mid X_{< j}, Y, J)$. Это равно
    $\frac{1}{n} \sum\limits_{j} I(X_j : \Pi \mid X_{< j}, Y, J = j) = \frac{1}{n} I(X : \Pi \mid Y)$.

    Единственное место, где мы пользовались тем, что наша мера является product measure --- это
    возможность насэмплить $X_{> j}$ и $Y_{> j}$ приватными случайными битами независимо друг от
    друга. Тогда изменим наш протокол следующим образом: теперь $X_{< j}$, $Y_{> j}$ --- это публичные
    случайные биты, а $X_{> j}$, $Y_{< j}$ --- приватные. Теперь приватные биты можно насэмплить в
    соответствии с нужной мерой, зная публичные. В доказательстве, соответственно, нужно только заменить
    $Y_{< j}$ на $Y_{> j}$. 
\end{proof}

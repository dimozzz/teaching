\chapter{Недетерминированная коммуникация}

\section{Определения}
Пусть $f\colon X\times Y\to \{0, 1\}$. Мы хотим определить недетерминированную коммуникационную сложность $f$.

\subsection{Первое определение}
Рассмотрим \emph{покрытия} матрицы $M_f$ прямоугольниками.

\begin{definition}
$C^1(f)$ "--- минимальное число 1-прямоугольников, покрывающих все единицы $M_f$.

$C^0(f)$ "--- минимальное число 0-прямоугольников, покрывающих все нули $M_f$.

Недетерминированная коммуникационная сложность функции $f$ "--- $\N^1(f) = \ceil{\log C^1(f)}$
\end{definition}

\subsection{Второе определение}
Три игрока: Алиса, Боб, Чарли.
Чарли посылает Алисе и Бобу одинаковую подсказку $w$.
Далее Алиса и Боб общаются как обычно.
Сложность: $|w| + \#(\text{переданных битов между Алисой и Бобом})$.
Ответ <<1>>, если существует подсказка $w$, для которой Алиса и Боб вычислят <<1>>. Ответ <<0>>, если для всех подсказок они вычислят <<0>>.

Вопросы:
\begin{itemize}
\item Может ли длина подсказки быть разной? Обычно мы фиксируем размер подсказки в протоколе.
\item Может ли протокол зависеть от $w$? Может, но утверждается, что это ни на что не влияет "--- но мы это не обосновывали. Дальше будем считать, что протокол не зависит от $w$.
\item Чарли знает $x$ и $y$? Это не важно, ведь у нас квантор существования по $w$.
\end{itemize}

Пример.
$\mathcal{NE}(x, y) = \One_{x\neq y}$.
Чарли посылает позицию различия. Алиса и Боб за два бита проверяют. Сложность: $\log n + 2$.

Почему мы хотим платить за $w$?
Пусть Чарли не платит. Тогда переделаем протокол так: Чарли теперь посылает не просто $w$, а $(w, l)$, где $l$ "--- лист протокола, в который приходили Алиса и Боб с подсказкой $w$. Тогда Алиса и Боб смогут проверить такую подсказку всего за два бита "--- что каждый приходит в такой лист. Таким образом, сложность любой функции будет не больше $2$, и определение теряет смысл.
(Или можно просто передать $(x, y)$ в качестве подсказки.)

\subsection{Третье определение}
Чарли посылает Алисе подсказку $w_x$, а Бобу "--- подсказку $w_y$. Далее Алиса и Боб общаются как обычно.
Сложность = $\#(\text{переданных битов между Алисой и Бобом})$, уже \emph{без платы} за подсказки.
Ответ <<1>>$\iff$ существует пара $(w_x, w_y)$, на которой Алиса и Боб выдадут <<1>>.

\subsection{Эквивалентность определений}
Покажем, что сложности в разных определениях совпадают с точностью до аддитивной константы.

\paragraph{Def1$\implies$Def2.}
Чарли посылает номер прямоугольника из $C^1(f)$, далее Алиса и Боб проверяют, что оба его пересекают. $\mathrm{Def}_2 \leqslant \mathrm{Def}_1 + 2$.

\paragraph{Def2$\implies$Def1.}
Возможных подсказок $2^{|w|}$.
Возможных пар (подсказка, лист с ответом 1) не более $2^{|w|+\#\text{bits}}$.
Каждой такой паре $(w, l)$ соответствует множество пар $A_{w,l} = (x, y)$, которые на подсказке $w$ приходят в лист $l$.
Заметим, что $x$ и $y$ можно проверять на принадлежность $A_{w, l}$ независимо, поэтому $A_{w,l}$ является прямоугольником.

Так что прямоугольники $\{A_{w, l}\}$ образуют покрытие единиц матрицы $M_f$.
$\mathrm{Def}_1 \leqslant \mathrm{Def}_2$.

\paragraph{Def1$\implies$Def3.}
Чарли посылает номер прямоугольника из $C^1(f)$. Алиса посылает Бобу свою подсказку, чтобы тот проверил, что они одинаковы, и они проверяют, что оба лежат в этом прямоугольнике (2 бита).
$\mathrm{Def}_3 \leqslant \mathrm{Def}_1 + 2$.

\paragraph{Def3$\implies$Def1.}
Каждый 1-лист даёт прямоугольник в матрице $(X\times W_x) \times (Y\times W_y)$, их не более $2^{\mathrm{Def}_3}$ штук.
Спроецируем такие прямоугольники в $X\times Y$.
Все единицы покрыты, ни в какие нули не попадём. Получили, что $C^1(f)\leqslant 2^{\mathrm{Def}_3}$.
$\mathrm{Def}_1\leqslant \mathrm{Def}_3$.

\section{Классы коммуникационной сложности}

\begin{definition}
$\mathcal{P}$ "--- класс функций, имеющих детерминированную сложность $\polylog(n)$, то есть $\D(f)\leqslant\polylog(n)$.

$\mathcal{NP}$ "--- класс функций, имеющих недетерминированную сложность $\polylog(n)$, то есть $\N^1(f)\leqslant\polylog(n)$.

$co\mathcal{NP}$ "--- класс функций, отрицания которых лежит в $\mathcal{NP}$, то есть $\N^0(f)\leqslant\polylog(n)$.
\end{definition}

\begin{remark}
$\mathcal{P} = \mathcal{NP}\cap co\mathcal{NP}$
\end{remark}
\begin{proof}
В теореме~\ref{D < log C0 * log C1} мы доказали, что $\D(f)\leqslant \O(\log C^0(f) \cdot \log C^1(f))$. Так что $\D(f)\leqslant \O(\N^0(f)\cdot \N^1(f))$, поэтому если $\N^0\leqslant\polylog$ и $\N^1\leqslant\polylog$, то $\D \leqslant \O(\polylog)$.
\end{proof}

\section{Rectangle Size для недетерминированной коммуникации}
Напомним, обычный принцип Rectangle Size (с мерой) утверждает, что если для любого прямоугольника $R$ выполняется $\mu(R) < \delta$, то $\D(f)\geqslant \log(1/\delta)$.

\begin{definition}
Пусть $a$ "--- какой-то цвет, $\mu$ "--- мера, дающая единицу на всех клетках этого цвета.

Определим $\RS^{\mu, a} = \max_{a\text{-rect}} \mu(R)$.

Тогда $\RS^a = \min_\mu \RS^{\mu, a}$ по всем таким мерам.
\end{definition}

\begin{theorem} [Второе неравенство "--- Wigderson]
$\frac{1}{\RS^a(f)}\leqslant C^a(f) \leqslant \O\left(\frac{n}{\RS^a(f)}\right)$.

И, следовательно,
$\log\left(\frac{1}{\RS^a(f)}\right)\leqslant \N^a(f) \leqslant \log\left(\frac{1}{\RS^a(f)}\right) + \log n + c$.
\end{theorem}
\begin{proof}
Первое неравенство очевидно.

Пусть $\mu_1$ "--- равномерная мера на $a$-элементах.
Есть $a$-прямоугольник $R$ такой, что $\mu_1(R)\geqslant \RS^a(f)$ (по определению $\RS^a$). Возьмём этот прямоугольник в набор.

Изменим меру. $\mu_2$ "--- равномерная на непокрытых $a$-элементах. Повторим.

Каждый раз покрываем хотя бы долю $\RS^a(f)$. Изначально $\leqslant 2^{2n}$ клеток. Хотим, чтобы $2^{2n}(1-\RS^a(f))^t < 1$.
Выбирая $t = \frac{2n}{\RS^a(f)}$, получаем требуемое.
\end{proof}

Заметим, что мы получили покрытие, но не обязательно разбиение.

Можно ещё сказать так: если есть нижняя оценка на $\N^a$, то есть мера, для которой Rectangle Size маленький.

\section{Недетерминированная сложность \texorpdfstring{$\DISJ^{\leqslant k}_n$}{DISJ(n, <=k)}}

Можно ли улучшить оценку $\D(f) \leqslant \O(\N^0(f)\cdot \N^1(f))$? Нет, она точная.
Рассмотрим $\DISJ^{\leqslant k}_n$, где множества имеют размеры не больше $k$.

Эта функция является <<всюду определённой>> в том смысле, что множество корректных входов образуют прямоугольник (то есть ограничения на вход Алисы и вход Боба независимы; иными словами, функцию можно записать как $f\colon X\times Y\to \{0, 1\}$). Это хорошо, потому что некоторым теоремам это было нужно.

\subsection{Верхняя оценка}

\begin{theorem}
$\N^0(\DISJ^{\leqslant k}_n) = \O(\log n)$.
\end{theorem}
\begin{proof}
$\N^0(\DISJ^{\leqslant k}_n) = \N^1\left(\overline{\DISJ^{\leqslant k}_n}\right) = \O(\log n)$ "--- общей подсказкой Чарли будет номер бита, в которой строки $x$ и $y$ пересекаются.
\end{proof}

\begin{theorem}
$\N^1(\DISJ^{\leqslant k}_n)\leqslant \O(k+\log\log n)$.
\end{theorem}
\begin{proof}
Наивно, без Чарли: $\O(k\log n)$ "--- передать позиции $\leqslant k$ единичек.

Пусть теперь Чарли посылает множество-разделитель (сепаратор) $S$ такое, что $x\subseteq S$, а $y\cap S=\emptyset$.
Проблема: потенциальных сепараторов слишком много, подсказка слишком большая. Так что выберем не слишком большое множество $Z$ потенциальных сепараторов, которые будут разделять любые два непересекающиеся множества размера $k$.
Докажем, что такое множество $Z$ существует, вероятностным методом.

Выберем $l$ случайных сепараторов.
Вероятность, что случайный сепаратор $S$ разделяет дизъюнктные $(x, y)$, не меньше $\frac{1}{2^{2k}}$.
Вероятность, что существует пара $(x, y)$, что ни один из выбранных сепараторов не разделяет $(x, y)$, не больше
$$\binom{n}{2k}\binom{2k}{k}\left(1-\frac{1}{2^{2k}}\right)^l \leqslant
n^{2k}\cdot 2^{2k}\cdot \left(1-\frac{1}{2^{2k}}\right)^l =
(2n)^{2k}\cdot \left(1-\frac{1}{2^{2k}}\right)^l.$$

$\log((2n)^{2k}) = 2k\log(2n)$, поэтому, выбирая $l = 2^{2k}\cdot (2k\log(2n))$, мы получим, что вероятность меньше единицы. Тогда найдётся набор из $l$ сепараторов, разделяющих любую пару $(x, y)$.

Теперь, чтобы передать номер одного из сепараторов, Чарли даёт подсказку из $\log l = 2k + \log(2k) + \log\log(2n) = \O(k + \log\log n)$ битов.
\end{proof}

Пусть теперь $k = \log n$. 
Тогда $\N^0 = \O(\log n)$, $\N^1 = \O(\log n)$. Для доказательства точности оценки осталось показать, что $\D(\DISJ_n^{\leqslant \log n})\geqslant \Omega(\log^2 n)$.